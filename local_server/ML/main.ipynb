{
 "cells": [
  {
   "cell_type": "code",
   "id": "d5b17099-c43b-482e-9c85-7d3300f5928d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install tensorflow\n",
    "%pip install numpy\n",
    "%pip install ipympl\n",
    "%pip install ipython\n",
    "%pip install pyarrow\n",
    "%pip install dask\n",
    "%pip install joblib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f6eff9-eb5f-4dee-bd4d-1f1201d8b52e",
   "metadata": {},
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "nblog = open(\"nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31cb98d3648c0add",
   "metadata": {},
   "source": [
    "# Training files\n",
    "TRAIN_FILES = [\n",
    "    \"conn.log.labeled\",\n",
    "    \"conn2.log.labeled\",\n",
    "    \"conn3.log.labeled\",\n",
    "    \"conn4.log.labeled\",\n",
    "    \"conn5.log.labeled\",\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"ts\",\n",
    "    \"uid\",\n",
    "    \"id.orig_h\",\n",
    "    \"id.orig_p\",\n",
    "    \"id.resp_h\",\n",
    "    \"id.resp_p\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f0383a3334ccf8b",
   "metadata": {},
   "source": [
    "for file in TRAIN_FILES:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        with open(f\"{file}.csv\", \"w\") as ff:\n",
    "            for line in lines[8:-1]:\n",
    "                line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "                ff.write(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4562663b-694e-4651-87e3-d617b6d9d163",
   "metadata": {},
   "source": [
    "# data_train = pd.concat([pd.read_csv(f'{file}.csv', names=columns) for file in TRAIN_FILES], ignore_index=True)\n",
    "\n",
    "\n",
    "def load_csv_parallel(file, columns):\n",
    "    \"\"\"Optimized CSV loading function for large files\"\"\"\n",
    "    chunks = pd.read_csv(\n",
    "        f\"{file}.csv\",\n",
    "        names=columns,\n",
    "        engine=\"c\",\n",
    "        low_memory=False,\n",
    "        memory_map=True,\n",
    "        cache_dates=True,\n",
    "        chunksize=1_000_000,  # Process 1M rows at a time\n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True, copy=False)\n",
    "\n",
    "\n",
    "# Optimize for 64 cores, leaving 2 cores free for system processes\n",
    "num_cores = 62\n",
    "\n",
    "# Create process pool\n",
    "with mp.Pool(num_cores) as pool:\n",
    "    # Map the loading function across files\n",
    "    dataframes = pool.starmap(\n",
    "        load_csv_parallel, [(file, columns) for file in TRAIN_FILES]\n",
    "    )\n",
    "\n",
    "# Combine all files' dataframes\n",
    "data_train = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "\n",
    "# Clear unneeded variables to free memory\n",
    "del dataframes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d3ddae86f202248",
   "metadata": {},
   "source": [
    "data_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ff2e8eee6b9fe7",
   "metadata": {},
   "source": [
    "data_train.describe().style.background_gradient(cmap=\"Blues\").set_properties(\n",
    "    **{\"font-family\": \"Segoe UI\"}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8597324da2b1307",
   "metadata": {},
   "source": [
    "def pie_plot(df, cols_list, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    for ax, col in zip(axes.ravel(), cols_list):\n",
    "        df[col].value_counts().plot(\n",
    "            ax=ax, kind=\"pie\", figsize=(15, 15), fontsize=10, autopct=\"%1.0f%%\"\n",
    "        )\n",
    "        ax.set_title(str(col), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pie_plot(data_train, [\"label\", \"proto\"], 1, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d014c100-0863-4fce-ae61-fdd974833451",
   "metadata": {},
   "source": [
    "def Scaling(df_num, cols):\n",
    "    \"\"\"Optimized scaling function with progress tracking\"\"\"\n",
    "    print(f\"Starting RobustScaler on {len(cols)} columns...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Initialize and fit scaler\n",
    "    scaler = RobustScaler(copy=True)\n",
    "    scaled_values = scaler.fit_transform(df_num)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=cols, index=df_num.index)\n",
    "    \n",
    "    print(f\"Scaling completed in {time.time() - t0:.2f}s\")\n",
    "    return scaled_df\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Optimized preprocessing pipeline with detailed progress tracking\"\"\"\n",
    "    print(\"\\n    Starting preprocessing pipeline...\")\n",
    "    print(f\"    Initial dataframe shape: {dataframe.shape}\")\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Define columns to process\n",
    "    cat_cols = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "    drop_cols = [\n",
    "       \"ts\", \"uid\", \"id.orig_h\", \"id.resp_h\", \"id.orig_p\", \n",
    "       \"id.resp_p\", \"tunnel_parents\", \"detailed-label\"\n",
    "    ]\n",
    "    \n",
    "    # 1. Drop unnecessary columns\n",
    "    print(\"\\n    [1/7] Dropping unnecessary columns...\")\n",
    "    t0 = time.time()\n",
    "    dataframe = dataframe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    print(f\"    Columns dropped in {time.time() - t0:.2f}s\")\n",
    "    print(f\"    Shape after dropping: {dataframe.shape}\")\n",
    "    \n",
    "    # 2. Replace dashes with NaN\n",
    "    print(\"\\n    [2/7] Replacing dashes with NaN...\")\n",
    "    t0 = time.time()\n",
    "    dataframe.replace(\"-\", np.nan, inplace=True)\n",
    "    print(f\"    Replacement completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # 3. Process numeric columns\n",
    "    print(\"\\n    [3/7] Processing numeric columns...\")\n",
    "    t0 = time.time()\n",
    "    numeric_cols = dataframe.columns.difference(cat_cols + [\"label\"])\n",
    "    print(f\"    Found {len(numeric_cols)} numeric columns\")\n",
    "    \n",
    "    # Convert to numeric in chunks for better memory usage\n",
    "    chunk_size = 5\n",
    "    for i in range(0, len(numeric_cols), chunk_size):\n",
    "       chunk_cols = numeric_cols[i:i+chunk_size]\n",
    "       for col in chunk_cols:\n",
    "           dataframe[col] = pd.to_numeric(dataframe[col], errors=\"coerce\")\n",
    "           \n",
    "    print(f\"    Numeric conversion completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # 4. Handle numeric data\n",
    "    print(\"\\n    [4/7] Processing numeric data and handling NaN values...\")\n",
    "    t0 = time.time()\n",
    "    df_num = dataframe[numeric_cols]\n",
    "    \n",
    "    # Check for all-NaN columns\n",
    "    all_nan_cols = df_num.columns[df_num.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "       print(f\"    Found {len(all_nan_cols)} columns with all NaN values\")\n",
    "       df_num[all_nan_cols] = 0\n",
    "    \n",
    "    # Impute missing values\n",
    "    print(\"Imputing missing values...\")\n",
    "    imputer = SimpleImputer(strategy=\"mean\", copy=False)\n",
    "    imputed_values = imputer.fit_transform(df_num)\n",
    "    df_num = pd.DataFrame(imputed_values, columns=numeric_cols, index=dataframe.index)\n",
    "    print(f\"    Numeric processing completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # 5. Scale numeric data\n",
    "    print(\"\\n    [5/7] Scaling numeric data...\")\n",
    "    scaled_df = Scaling(df_num, df_num.columns)\n",
    "    dataframe[df_num.columns] = scaled_df.values\n",
    "    del scaled_df  # Free memory\n",
    "    \n",
    "    # 6. Convert labels\n",
    "    print(\"\\n    [6/7] Converting labels...\")\n",
    "    t0 = time.time()\n",
    "    dataframe[\"label\"] = (dataframe[\"label\"] != \"Benign\").astype(np.int8)  # More efficient than lambda\n",
    "    print(f\"    Label conversion completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # 7. One-hot encode categorical columns\n",
    "    print(\"\\n    [7/7] One-hot encoding categorical columns...\")\n",
    "    t0 = time.time()\n",
    "    dataframe = pd.get_dummies(dataframe, columns=cat_cols, drop_first=True, sparse=False)\n",
    "    print(f\"    One-hot encoding completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(f\"\\n    Final dataframe shape: {dataframe.shape}\")\n",
    "    print(f\"    Memory usage: {dataframe.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"    Total preprocessing time: {time.time() - t_start:.2f}s\")\n",
    "    \n",
    "    return dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db9e92dc-f200-4a77-ac63-bf61363e555b",
   "metadata": {},
   "source": [
    "print(f\"[1/7] Starting preprocessing pipeline...\")\n",
    "print(f\"Input shape: {data_train.shape}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Preprocess data with progress tracking\n",
    "print(\"[2/7] Preprocessing data...\")\n",
    "scaled_train = preprocess(data_train)\n",
    "print(f\"Preprocessing completed in {time.time() - t0:.2f}s\")\n",
    "print(f\"Preprocessed shape: {scaled_train.shape}\")\n",
    "\n",
    "# Convert to float32 and split features/target\n",
    "print(\"[3/7] Converting features to float32...\")\n",
    "t1 = time.time()\n",
    "x = scaled_train.drop([\"label\"], axis=1, errors=\"ignore\").values\n",
    "x = np.asarray(x, dtype=np.float32)  # More efficient than .astype()\n",
    "print(f\"Features conversion completed in {time.time() - t1:.2f}s\")\n",
    "print(f\"Features shape: {x.shape}\")\n",
    "\n",
    "print(\"[4/7] Converting labels to int32...\")\n",
    "t2 = time.time()\n",
    "y = np.asarray(scaled_train[\"label\"].values, dtype=np.int32)  # int32 is usually sufficient\n",
    "print(f\"Labels conversion completed in {time.time() - t2:.2f}s\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Free memory\n",
    "print(\"[5/7] Clearing unused data to free memory...\")\n",
    "del scaled_train\n",
    "del data_train\n",
    "\n",
    "# PCA with progress info\n",
    "print(\"[6/7] Performing PCA reduction to 20 components...\")\n",
    "t3 = time.time()\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "x_reduced = pca.fit_transform(x)\n",
    "print(f\"PCA completed in {time.time() - t3:.2f}s\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"Reduced features shape: {x_reduced.shape}\")\n",
    "\n",
    "# Free more memory\n",
    "del x\n",
    "\n",
    "# Train test split\n",
    "print(\"[7/7] Performing train-test split...\")\n",
    "t4 = time.time()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "   x_reduced, \n",
    "   y, \n",
    "   test_size=0.2, \n",
    "   random_state=42,\n",
    "   shuffle=True,  # Explicit shuffle\n",
    "   stratify=y     # Maintain label distribution\n",
    ")\n",
    "print(f\"Split completed in {time.time() - t4:.2f}s\")\n",
    "\n",
    "# Print final shapes\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTotal pipeline time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "# Memory cleanup\n",
    "del x_reduced\n",
    "    \n",
    "# Save PCA model\n",
    "print(\"\\nSaving PCA model...\")\n",
    "joblib.dump(pca, \"pca_model.joblib\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68a875bdb63b7c04",
   "metadata": {},
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "def create_packet_classifier(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Creates an optimized deep learning model for network packet classification\n",
    "    with improved architecture and regularization.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Shape of input features\n",
    "        num_classes: Number of output classes (default 1 for binary classification)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize regularization parameters\n",
    "    reg_config = {\n",
    "        \"kernel\": regularizers.L1L2(l1=1e-6, l2=1e-5),\n",
    "        \"bias\": regularizers.L2(1e-5),\n",
    "        \"activity\": regularizers.L2(1e-6),\n",
    "    }\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Input layer with batch normalization\n",
    "            layers.InputLayer(input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            # First block - smaller layers for feature extraction\n",
    "            layers.Dense(\n",
    "                32,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            # Second block - moderate size for pattern recognition\n",
    "            layers.Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            # Third block - larger size for complex pattern learning\n",
    "            layers.Dense(\n",
    "                256,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            # Output layer\n",
    "            layers.Dense(num_classes, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, batch_size=32, max_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains the model with optimized parameters and callbacks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping to prevent overfitting\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        # Reduce learning rate when training plateaus\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    "        ),\n",
    "        # Save best model with .keras extension\n",
    "        ModelCheckpoint(\n",
    "            \"best_packet_classifier.keras\",  # Changed from .h5 to .keras\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    ]\n",
    "\n",
    "    # Compile model with optimized parameters\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3,\n",
    "        clipnorm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def preprocess_data(columns, data):\n",
    "    \"\"\"\n",
    "    Preprocesses the network packet data.\n",
    "    \"\"\"\n",
    "    # Convert categorical columns to numerical\n",
    "    categorical_columns = [\n",
    "        \"proto\",\n",
    "        \"service\",\n",
    "        \"conn_state\",\n",
    "        \"history\",\n",
    "        \"tunnel_parents\",\n",
    "    ]\n",
    "    numerical_columns = [\n",
    "        col\n",
    "        for col in columns\n",
    "        if col not in categorical_columns + [\"label\", \"detailed-label\"]\n",
    "    ]\n",
    "\n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        data = pd.get_dummies(data, columns=[col], prefix=col)\n",
    "\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c484f89da5be307",
   "metadata": {},
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "print(\"Shaped\")\n",
    "model = create_packet_classifier(input_shape)\n",
    "print(\"model\")\n",
    "history = train_model(model, x_train, y_train, x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce5c4b0a8d5ec071",
   "metadata": {},
   "source": [
    "with open(\"test3.log.labeled\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(\"test3.log.labeled.csv\", \"w\") as ff:\n",
    "        for line in lines[8:-1]:\n",
    "            line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "            ff.write(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43320917c0491b7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "df = pd.read_csv(\"test3.log.labeled.csv\")\n",
    "\n",
    "columns = [\n",
    "    \"ts\",\n",
    "    \"uid\",\n",
    "    \"id.orig_h\",\n",
    "    \"id.orig_p\",\n",
    "    \"id.resp_h\",\n",
    "    \"id.resp_p\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "required_columns = [\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"conn_state\",\n",
    "    \"history\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "df_new = df.copy()\n",
    "\n",
    "# Add history column with default value if missing\n",
    "if \"history\" not in df.columns:\n",
    "    df[\"history\"] = \"S\"\n",
    "\n",
    "\n",
    "# Add label column (will be dropped during preprocessing)\n",
    "df[\"label\"] = \"unknown\"\n",
    "\n",
    "# Preprocess the data using your existing preprocess function\n",
    "preprocessed_data = preprocess(df)\n",
    "\n",
    "# Ensure columns match training data\n",
    "train_columns = scaled_train.drop(\"label\", axis=1).columns\n",
    "missing_cols = set(train_columns) - set(preprocessed_data.columns)\n",
    "for col in missing_cols:\n",
    "    preprocessed_data[col] = 0\n",
    "\n",
    "# Reorder columns to match training data\n",
    "preprocessed_data = preprocessed_data[train_columns]\n",
    "\n",
    "# Convert to numpy array\n",
    "x_new = preprocessed_data.values.astype(\"float32\")\n",
    "\n",
    "# Transform with PCA\n",
    "x_new_reduced = pca.transform(x_new)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_new_reduced)\n",
    "predicted_outcomes = (predictions > 0.5).astype(\"int\")\n",
    "\n",
    "# Add predictions to original DataFrame\n",
    "df[\"predicted_outcome\"] = predicted_outcomes\n",
    "\n",
    "# Calculate basic statistics\n",
    "total_packets = len(predicted_outcomes)\n",
    "malicious_packets = np.sum(predicted_outcomes)\n",
    "normal_packets = total_packets - malicious_packets\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "print(\"Prediction Summary:\")\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Actual summary\n",
    "print(\"Summary of actual data\")\n",
    "print(\"Total packets: \", len(df_new))\n",
    "print(\"Unique protocols: \", df_new[\"proto\"].nunique())\n",
    "\n",
    "malicious_packets = df_new[\"label\"].value_counts()[0]\n",
    "normal_packets = df_new[\"label\"].value_counts()[1]\n",
    "total_packets = len(df_new)\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\" \")\n",
    "# Print the statistics\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "pie_plot(df_new, [\"label\", \"proto\"], 1, 2)\n",
    "\n",
    "\n",
    "# Create time-based analysis\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"ts\"], unit=\"s\")\n",
    "df[\"minute\"] = df[\"timestamp\"].dt.floor(\"T\")\n",
    "\n",
    "# Analyze predictions over time\n",
    "temporal_analysis = (\n",
    "    df.groupby(\"minute\").agg({\"predicted_outcome\": [\"count\", \"sum\"]}).reset_index()\n",
    ")\n",
    "temporal_analysis.columns = [\"minute\", \"total_packets\", \"malicious_packets\"]\n",
    "temporal_analysis[\"normal_packets\"] = (\n",
    "    temporal_analysis[\"total_packets\"] - temporal_analysis[\"malicious_packets\"]\n",
    ")\n",
    "temporal_analysis[\"malicious_ratio\"] = (\n",
    "    temporal_analysis[\"malicious_packets\"] / temporal_analysis[\"total_packets\"]\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "plt.style.use(\"default\")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Timeline of predictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"normal_packets\"],\n",
    "    label=\"Normal\",\n",
    "    color=\"#2ecc71\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"malicious_packets\"],\n",
    "    label=\"Malicious\",\n",
    "    color=\"#e74c3c\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.title(\"Packet Classification Over Time\", pad=20, fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\", fontsize=10)\n",
    "plt.ylabel(\"Number of Packets\", fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Plot 2: Malicious ratio over time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"malicious_ratio\"],\n",
    "    color=\"#9b59b6\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.title(\n",
    "    \"Ratio of Malicious Packets Over Time\", pad=20, fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Time\", fontsize=10)\n",
    "plt.ylabel(\"Ratio of Malicious Packets\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(\"data/predicted_new_data.csv\", index=False)\n",
    "temporal_analysis.to_csv(\"data/temporal_analysis.csv\", index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"total_packets\": total_packets,\n",
    "    \"malicious_packets\": int(malicious_packets),\n",
    "    \"normal_packets\": int(normal_packets),\n",
    "    \"malicious_percentage\": float(malicious_percentage),\n",
    "    \"analysis_timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"available_features\": df.columns.tolist(),\n",
    "}\n",
    "\n",
    "with open(\"data/verification_results.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to 'data' directory\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
