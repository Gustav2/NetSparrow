{
 "cells": [
  {
   "cell_type": "code",
   "id": "d5b17099-c43b-482e-9c85-7d3300f5928d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install tensorflow\n",
    "%pip install numpy\n",
    "%pip install ipympl\n",
    "%pip install ipython\n",
    "%pip install pyarrow\n",
    "%pip install dask\n",
    "%pip install joblib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5acd9b7-c680-46fc-9255-2fb7f56d8bef",
   "metadata": {},
   "source": [
    "%pip install seaborn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4f6eff9-eb5f-4dee-bd4d-1f1201d8b52e",
   "metadata": {},
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "nblog = open(\"nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31cb98d3648c0add",
   "metadata": {},
   "source": [
    "# Training files\n",
    "TRAIN_FILES = [\n",
    "    \"conn.log.labeled\",\n",
    "    \"conn2.log.labeled\",\n",
    "    \"conn3.log.labeled\",\n",
    "    # \"conn4.log.labeled\",\n",
    "    \"conn5.log.labeled\",\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"ts\",\n",
    "    \"uid\",\n",
    "    \"id.orig_h\",\n",
    "    \"id.orig_p\",\n",
    "    \"id.resp_h\",\n",
    "    \"id.resp_p\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f0383a3334ccf8b",
   "metadata": {},
   "source": [
    "for file in TRAIN_FILES:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        with open(f\"{file}.csv\", \"w\") as ff:\n",
    "            for line in lines[8:-1]:\n",
    "                line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "                ff.write(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4562663b-694e-4651-87e3-d617b6d9d163",
   "metadata": {},
   "source": [
    "def load_csv_parallel(file, columns):\n",
    "    \"\"\"Optimized CSV loading function for large files\"\"\"\n",
    "    chunks = pd.read_csv(\n",
    "        f\"{file}.csv\",\n",
    "        names=columns,\n",
    "        engine=\"c\",\n",
    "        low_memory=False,\n",
    "        memory_map=True,\n",
    "        cache_dates=True,\n",
    "        chunksize=1_000_000, \n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True, copy=False)\n",
    "\n",
    "num_cores = 62\n",
    "\n",
    "with mp.Pool(num_cores) as pool:\n",
    "    dataframes = pool.starmap(\n",
    "        load_csv_parallel, [(file, columns) for file in TRAIN_FILES]\n",
    "    )\n",
    "\n",
    "data_train = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "\n",
    "del dataframes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d3ddae86f202248",
   "metadata": {},
   "source": [
    "data_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ff2e8eee6b9fe7",
   "metadata": {},
   "source": [
    "data_train.describe().style.background_gradient(cmap=\"Blues\").set_properties(\n",
    "    **{\"font-family\": \"Segoe UI\"}\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8597324da2b1307",
   "metadata": {},
   "source": [
    "def pie_plot(df, cols_list, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    for ax, col in zip(axes.ravel(), cols_list):\n",
    "        df[col].value_counts().plot(\n",
    "            ax=ax, kind=\"pie\", figsize=(15, 15), fontsize=10, autopct=\"%1.0f%%\"\n",
    "        )\n",
    "        ax.set_title(str(col), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pie_plot(data_train, [\"detailed-label\", \"proto\"], 1, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b4b8596-67ef-4440-9bde-cbdf1af5d606",
   "metadata": {},
   "source": [
    "def improved_distribution_plot(df, cols_list, rows, cols):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "    for ax, col in zip(axes.ravel(), cols_list):\n",
    "        if col == \"detailed-label\":\n",
    "            df = df.copy() \n",
    "            df[col] = df[col].replace(\"-\", \"Benign\")\n",
    "\n",
    "        counts = df[col].value_counts()\n",
    "\n",
    "        if col == \"detailed-label\":\n",
    "            total = counts.sum()\n",
    "            small_protocols = counts[counts / total < 0.01]\n",
    "            main_protocols = counts[counts / total >= 0.01]\n",
    "\n",
    "            if not small_protocols.empty:\n",
    "                pass  # main_protocols['Others'] = small_protocols.sum()\n",
    "\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(main_protocols)))\n",
    "            explode = [0.05] * len(main_protocols)\n",
    "            wedges, texts, autotexts = ax.pie(\n",
    "                main_protocols,\n",
    "                explode=explode,\n",
    "                labels=main_protocols.index,\n",
    "                colors=colors,\n",
    "                autopct=\"%1.1f%%\",\n",
    "                pctdistance=0.85,\n",
    "            )\n",
    "\n",
    "            ax.legend(\n",
    "                wedges,\n",
    "                main_protocols.index,\n",
    "                title=\"Attack Types\",\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1, 0.7),\n",
    "            )\n",
    "\n",
    "        else: \n",
    "            colors = [\"lightblue\", \"lightcoral\"]\n",
    "            wedges, texts, autotexts = ax.pie(\n",
    "                counts,\n",
    "                labels=counts.index,\n",
    "                colors=colors,\n",
    "                autopct=\"%1.1f%%\",\n",
    "                explode=[0.05] * len(counts),\n",
    "            )\n",
    "\n",
    "            ax.legend(\n",
    "                wedges,\n",
    "                counts.index,\n",
    "                title=col.capitalize(),\n",
    "                loc=\"center left\",\n",
    "                bbox_to_anchor=(1, 0.5),\n",
    "            )\n",
    "\n",
    "        plt.setp(autotexts, size=9, weight=\"bold\")\n",
    "        plt.setp(texts, size=10)\n",
    "\n",
    "        ax.set_title(f\"{col.capitalize()} Distribution\", fontsize=12, pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for col in cols_list:\n",
    "        if col == \"detailed-label\":\n",
    "            counts = df[col].value_counts()\n",
    "        else:\n",
    "            counts = df[col].value_counts()\n",
    "        print(f\"\\n{col.capitalize()} Distribution:\")\n",
    "        print(\"-\" * 30)\n",
    "        for idx, value in counts.items():\n",
    "            print(f\"{idx}: {value:,} ({value/len(df)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "improved_distribution_plot(data_train, [\"detailed-label\", \"proto\"], 2, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d014c100-0863-4fce-ae61-fdd974833451",
   "metadata": {},
   "source": [
    "def Scaling(df_num, cols):\n",
    "    \"\"\"Optimized scaling function with progress tracking\"\"\"\n",
    "    print(f\"Starting RobustScaler on {len(cols)} columns...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    scaler = RobustScaler(copy=True)\n",
    "    scaled_values = scaler.fit_transform(df_num)\n",
    "    \n",
    "    scaled_df = pd.DataFrame(scaled_values, columns=cols, index=df_num.index)\n",
    "    \n",
    "    print(f\"Scaling completed in {time.time() - t0:.2f}s\")\n",
    "    return scaled_df\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    \"\"\"Optimized preprocessing pipeline with detailed progress tracking\"\"\"\n",
    "    print(\"\\n    Starting preprocessing pipeline...\")\n",
    "    print(f\"    Initial dataframe shape: {dataframe.shape}\")\n",
    "    t_start = time.time()\n",
    "    \n",
    "    cat_cols = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "    drop_cols = [\n",
    "       \"ts\", \"uid\", \"id.orig_h\", \"id.resp_h\", \"id.orig_p\", \n",
    "       \"id.resp_p\", \"tunnel_parents\", \"detailed-label\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n    [1/7] Dropping unnecessary columns...\")\n",
    "    t0 = time.time()\n",
    "    dataframe = dataframe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    print(f\"    Columns dropped in {time.time() - t0:.2f}s\")\n",
    "    print(f\"    Shape after dropping: {dataframe.shape}\")\n",
    "    \n",
    "    print(\"\\n    [2/7] Replacing dashes with NaN...\")\n",
    "    t0 = time.time()\n",
    "    dataframe.replace(\"-\", np.nan, inplace=True)\n",
    "    print(f\"    Replacement completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    print(\"\\n    [3/7] Processing numeric columns...\")\n",
    "    t0 = time.time()\n",
    "    numeric_cols = dataframe.columns.difference(cat_cols + [\"label\"])\n",
    "    print(f\"    Found {len(numeric_cols)} numeric columns\")\n",
    "\n",
    "    chunk_size = 5\n",
    "    for i in range(0, len(numeric_cols), chunk_size):\n",
    "       chunk_cols = numeric_cols[i:i+chunk_size]\n",
    "       for col in chunk_cols:\n",
    "           dataframe[col] = pd.to_numeric(dataframe[col], errors=\"coerce\")\n",
    "           \n",
    "    print(f\"    Numeric conversion completed in {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    print(\"\\n    [4/7] Processing numeric data and handling NaN values...\")\n",
    "    t0 = time.time()\n",
    "    df_num = dataframe[numeric_cols]\n",
    "\n",
    "    all_nan_cols = df_num.columns[df_num.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "       print(f\"    Found {len(all_nan_cols)} columns with all NaN values\")\n",
    "       df_num[all_nan_cols] = 0\n",
    "\n",
    "    print(\"Imputing missing values...\")\n",
    "    imputer = SimpleImputer(strategy=\"mean\", copy=False)\n",
    "    imputed_values = imputer.fit_transform(df_num)\n",
    "    df_num = pd.DataFrame(imputed_values, columns=numeric_cols, index=dataframe.index)\n",
    "    print(f\"    Numeric processing completed in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(\"\\n    [5/7] Scaling numeric data...\")\n",
    "    scaled_df = Scaling(df_num, df_num.columns)\n",
    "    dataframe[df_num.columns] = scaled_df.values\n",
    "    del scaled_df  # Free memory\n",
    "\n",
    "    print(\"\\n    [6/7] Converting labels...\")\n",
    "    t0 = time.time()\n",
    "    dataframe[\"label\"] = (dataframe[\"label\"] != \"Benign\").astype(np.int8)  # More efficient than lambda\n",
    "    print(f\"    Label conversion completed in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(\"\\n    [7/7] One-hot encoding categorical columns...\")\n",
    "    t0 = time.time()\n",
    "    dataframe = pd.get_dummies(dataframe, columns=cat_cols, drop_first=True, sparse=False)\n",
    "    print(f\"    One-hot encoding completed in {time.time() - t0:.2f}s\")\n",
    "\n",
    "    print(f\"\\n    Final dataframe shape: {dataframe.shape}\")\n",
    "    print(f\"    Memory usage: {dataframe.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"    Total preprocessing time: {time.time() - t_start:.2f}s\")\n",
    "    \n",
    "    return dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db9e92dc-f200-4a77-ac63-bf61363e555b",
   "metadata": {},
   "source": [
    "print(f\"[1/7] Starting preprocessing pipeline...\")\n",
    "print(f\"Input shape: {data_train.shape}\")\n",
    "t0 = time.time()\n",
    "\n",
    "print(\"[2/7] Preprocessing data...\")\n",
    "scaled_train = preprocess(data_train)\n",
    "print(f\"Preprocessing completed in {time.time() - t0:.2f}s\")\n",
    "print(f\"Preprocessed shape: {scaled_train.shape}\")\n",
    "\n",
    "print(\"[3/7] Converting features to float32...\")\n",
    "t1 = time.time()\n",
    "x = scaled_train.drop([\"label\"], axis=1, errors=\"ignore\").values\n",
    "x = np.asarray(x, dtype=np.float32)  # More efficient than .astype()\n",
    "print(f\"Features conversion completed in {time.time() - t1:.2f}s\")\n",
    "print(f\"Features shape: {x.shape}\")\n",
    "\n",
    "print(\"[4/7] Converting labels to int32...\")\n",
    "t2 = time.time()\n",
    "y = np.asarray(scaled_train[\"label\"].values, dtype=np.int32)  # int32 is sufficient\n",
    "print(f\"Labels conversion completed in {time.time() - t2:.2f}s\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "print(\"[5/7] Clearing unused data to free memory...\")\n",
    "del scaled_train\n",
    "del data_train\n",
    "\n",
    "print(\"[6/7] Performing PCA reduction...\")\n",
    "t3 = time.time()\n",
    "\n",
    "feature_variance = np.var(x, axis=0)\n",
    "low_var_features = np.sum(feature_variance < 1e-6)\n",
    "if low_var_features > 0:\n",
    "    print(f\"Warning: {low_var_features} features have very low variance\")\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)  \n",
    "x_reduced = pca.fit_transform(x_scaled)\n",
    "\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "print(f\"PCA completed in {time.time() - t3:.2f}s\")\n",
    "print(\"\\nPCA Variance Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total variance preserved: {pca.explained_variance_ratio_.sum():.3%}\")\n",
    "print(f\"Number of components needed for 95% variance: {pca.n_components_}\")\n",
    "print(f\"Original dimensionality: {x.shape[1]}\")\n",
    "print(f\"Dimensionality reduction ratio: {pca.n_components_/x.shape[1]:.3%}\")\n",
    "\n",
    "# Print detailed component analysis\n",
    "print(\"\\nTop 5 components variance explanation:\")\n",
    "for i in range(min(5, pca.n_components_)):\n",
    "    print(\n",
    "        f\"Component {i+1}: {pca.explained_variance_ratio_[i]:.3%} \"\n",
    "        f\"(Cumulative: {cumulative_variance_ratio[i]:.3%})\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nReduced features shape: {x_reduced.shape}\")\n",
    "\n",
    "# Train test split\n",
    "print(\"\\n[7/7] Performing train-test split...\")\n",
    "t4 = time.time()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_reduced, y, test_size=0.2, random_state=42, shuffle=True, stratify=y\n",
    ")\n",
    "print(f\"Split completed in {time.time() - t4:.2f}s\")\n",
    "\n",
    "# Print final shapes\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "print(f\"\\nTotal pipeline time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "# Memory cleanup\n",
    "del x_reduced\n",
    "del x\n",
    "del x_scaled\n",
    "\n",
    "# Save PCA model with scaler\n",
    "print(\"\\nSaving PCA model and scaler...\")\n",
    "joblib.dump({\"pca\": pca, \"scaler\": scaler}, \"pca_model.joblib\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68a875bdb63b7c04",
   "metadata": {},
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "def create_packet_classifier(input_shape, num_classes=1):\n",
    "    reg_config = {\n",
    "        \"kernel\": regularizers.L1L2(l1=1e-6, l2=1e-5),\n",
    "        \"bias\": regularizers.L2(1e-5),\n",
    "        \"activity\": regularizers.L2(1e-6),\n",
    "    }\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "\n",
    "            layers.Dense(\n",
    "                32,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "\n",
    "            layers.Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "\n",
    "            layers.Dense(\n",
    "                256,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "\n",
    "            layers.Dense(num_classes, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, batch_size=32, max_epochs=50):\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            \"best_packet_classifier.keras\",  \n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    ]\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3,\n",
    "        clipnorm=1.0, \n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c484f89da5be307",
   "metadata": {},
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "print(\"Shaped\")\n",
    "model = create_packet_classifier(input_shape)\n",
    "print(\"model\")\n",
    "history = train_model(model, x_train, y_train, x_test, y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce5c4b0a8d5ec071",
   "metadata": {},
   "source": [
    "with open(\"34.conn.log.labeled\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(\"34.conn.log.labeled.csv\", \"w\") as ff:\n",
    "        for line in lines[8:-1]:\n",
    "            line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "            ff.write(line)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43320917c0491b7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (confusion_matrix, roc_curve, precision_recall_curve, \n",
    "                           f1_score, auc, accuracy_score, precision_score, recall_score)\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load and preprocess data\n",
    "#df = pd.read_csv(\"test3.log.labeled.csv\")\n",
    "\n",
    "\n",
    "def load_csv_parallel(file, columns):\n",
    "    chunks = pd.read_csv(\n",
    "        f\"{file}.csv\",\n",
    "        names=columns,\n",
    "        engine=\"c\",\n",
    "        low_memory=False,\n",
    "        memory_map=True,\n",
    "        cache_dates=True,\n",
    "        chunksize=1_000_000,\n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True, copy=False)\n",
    "\n",
    "num_cores = 62\n",
    "\n",
    "with mp.Pool(num_cores) as pool:\n",
    "    dataframes = pool.starmap(\n",
    "        load_csv_parallel, [(file, columns) for file in [\"8.conn.log.labeled\", \"34.conn.log.labeled\"]]\n",
    "    )\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "\n",
    "del dataframes\n",
    "\n",
    "columns = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\",\n",
    "    \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\",\n",
    "    \"resp_ip_bytes\", \"tunnel_parents\", \"label\", \"detailed-label\"\n",
    "]\n",
    "\n",
    "df.columns = columns\n",
    "required_columns = [\n",
    "    \"proto\", \"service\", \"conn_state\", \"history\", \"local_orig\", \"local_resp\",\n",
    "    \"orig_bytes\", \"resp_bytes\", \"orig_ip_bytes\", \"resp_ip_bytes\", \"resp_pkts\"\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# Store actual labels\n",
    "actual_labels = df['label'].map({'Benign': 0, 'Malicious': 1}).fillna(0).astype(int)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_data = preprocess(df)\n",
    "\n",
    "# Load model and make predictions\n",
    "model = load_model(\"best_packet_classifier.keras\")\n",
    "\n",
    "# Perform PCA\n",
    "print(\"Performing PCA reduction...\")\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "x_new = preprocessed_data.values.astype(\"float32\")\n",
    "x_new_reduced = pca.fit_transform(x_new)\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Get prediction probabilities\n",
    "predictions_prob = model.predict(x_new_reduced)\n",
    "\n",
    "# Create a more comprehensive threshold analysis\n",
    "thresholds = np.linspace(0.1, 0.99, 90)  # Test more values from 0.1 to 0.99\n",
    "metrics = []\n",
    "\n",
    "print(\"Analyzing different threshold values...\")\n",
    "for threshold in thresholds:\n",
    "    predictions = (predictions_prob > threshold).astype(\"int\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(actual_labels, predictions)\n",
    "    prec = precision_score(actual_labels, predictions)\n",
    "    rec = recall_score(actual_labels, predictions)\n",
    "    f1 = f1_score(actual_labels, predictions)\n",
    "    \n",
    "    # Calculate true and false positive rates\n",
    "    tn, fp, fn, tp = confusion_matrix(actual_labels, predictions).ravel()\n",
    "    tpr = tp / (tp + fn)  # Recall\n",
    "    fpr = fp / (fp + tn)\n",
    "    \n",
    "    metrics.append({\n",
    "        'threshold': threshold,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'true_positive_rate': tpr,\n",
    "        'false_positive_rate': fpr,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot comprehensive metrics across thresholds\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['accuracy'], label='Accuracy', linewidth=2)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['precision'], label='Precision', linewidth=2)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['recall'], label='Recall', linewidth=2)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['f1_score'], label='F1 Score', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Model Metrics vs Confidence Threshold',\n",
    "          pad=20, fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "# Plot true positives and false positives\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['true_positives'], \n",
    "         label='True Positives', color='green', linewidth=2)\n",
    "plt.plot(metrics_df['threshold'], metrics_df['false_positives'], \n",
    "         label='False Positives', color='red', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Classification Counts vs Confidence Threshold',\n",
    "          pad=20, fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Confidence Threshold')\n",
    "plt.ylabel('Number of Predictions')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold based on different criteria\n",
    "optimal_f1_threshold = metrics_df.loc[metrics_df['f1_score'].idxmax(), 'threshold']\n",
    "optimal_accuracy_threshold = metrics_df.loc[metrics_df['accuracy'].idxmax(), 'threshold']\n",
    "optimal_precision_threshold = metrics_df.loc[metrics_df['precision'].idxmax(), 'threshold']\n",
    "\n",
    "# Print optimal thresholds and their metrics\n",
    "print(\"\\nOptimal Thresholds Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Optimal threshold (F1 Score): {optimal_f1_threshold:.3f}\")\n",
    "print(f\"Optimal threshold (Accuracy): {optimal_accuracy_threshold:.3f}\")\n",
    "print(f\"Optimal threshold (Precision): {optimal_precision_threshold:.3f}\")\n",
    "\n",
    "# Let's find a threshold that balances precision and recall\n",
    "balance_idx = (metrics_df['precision'] - metrics_df['recall']).abs().idxmin()\n",
    "balanced_threshold = metrics_df.loc[balance_idx, 'threshold']\n",
    "print(f\"Balanced threshold (Precision ≈ Recall): {balanced_threshold:.3f}\")\n",
    "\n",
    "print(\"\\nMetrics at different thresholds:\")\n",
    "thresholds_to_show = [0.1, 0.3, 0.5, 0.7, 0.85, 0.9, 0.925, 0.95, 0.99]\n",
    "print(\"\\nThreshold  Accuracy  Precision  Recall  F1-Score  TP  FP\")\n",
    "print(\"-\" * 65)\n",
    "for t in thresholds_to_show:\n",
    "    row = metrics_df[metrics_df['threshold'].round(2) == round(t, 2)].iloc[0]\n",
    "    print(f\"{t:9.2f}  {row['accuracy']:8.3f}  {row['precision']:9.3f}  \"\n",
    "          f\"{row['recall']:6.3f}  {row['f1_score']:8.3f}  \"\n",
    "          f\"{int(row['true_positives']):3d}  {int(row['false_positives']):3d}\")\n",
    "\n",
    "# Use threshold of 0.9 as recommended\n",
    "chosen_threshold = 0.9\n",
    "predictions = (predictions_prob > chosen_threshold).astype(\"int\")\n",
    "\n",
    "# Create confusion matrix with chosen threshold\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "# Create labeled confusion matrix\n",
    "labels = ['Benign', 'Malicious']\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "# Calculate percentages for annotations\n",
    "cm_norm = cm.astype('float') / cm.sum()\n",
    "annotations = np.array([f'{count}\\n({percentage:.1%})'\n",
    "                       for count, percentage in zip(cm.flatten(), cm_norm.flatten())])\n",
    "annotations = annotations.reshape(cm.shape)\n",
    "\n",
    "# Plot confusion matrix with better styling\n",
    "sns.heatmap(cm_df, \n",
    "            annot=annotations,\n",
    "            fmt='',\n",
    "            cmap='Blues',\n",
    "            square=True,\n",
    "            cbar=True)\n",
    "\n",
    "plt.title(f'Confusion Matrix\\nConfidence Threshold: {chosen_threshold:.3f}', \n",
    "          pad=20, fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "fpr, tpr, _ = roc_curve(actual_labels, predictions_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', \n",
    "          pad=20, fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Protocol-wise analysis\n",
    "protocol_comparison = pd.DataFrame({\n",
    "    'Protocol': df['proto'],\n",
    "    'Actual': actual_labels,\n",
    "    'Predicted': predictions.flatten()\n",
    "})\n",
    "\n",
    "protocol_stats = protocol_comparison.groupby('Protocol').agg({\n",
    "    'Actual': ['mean', 'count'],\n",
    "    'Predicted': 'mean'\n",
    "})\n",
    "\n",
    "protocol_stats.columns = ['Actual_Rate', 'Count', 'Predicted_Rate']\n",
    "protocol_stats = protocol_stats.sort_values('Count', ascending=False)\n",
    "\n",
    "# Convert to percentages\n",
    "protocol_stats['Actual_Rate'] *= 100\n",
    "protocol_stats['Predicted_Rate'] *= 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = protocol_stats[['Actual_Rate', 'Predicted_Rate']].plot(\n",
    "    kind='bar',\n",
    "    rot=45,\n",
    "    width=0.8\n",
    ")\n",
    "\n",
    "plt.title(f'Protocol-wise Classification Comparison\\nConfidence Threshold: {chosen_threshold:.3f}', \n",
    "          pad=20, fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Protocol (sorted by frequency)')\n",
    "plt.ylabel('Percentage Classified as Malicious')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Add count annotations\n",
    "for i, (idx, row) in enumerate(protocol_stats.iterrows()):\n",
    "    plt.text(i, max(row['Actual_Rate'], row['Predicted_Rate']) + 1,\n",
    "             f'n={int(row[\"Count\"])}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final performance metrics\n",
    "print(\"\\nFinal Model Performance Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Confidence Threshold: {chosen_threshold:.3f}\")\n",
    "print(f\"Accuracy: {accuracy_score(actual_labels, predictions):.4f}\")\n",
    "print(f\"Precision: {precision_score(actual_labels, predictions):.4f}\")\n",
    "print(f\"Recall: {recall_score(actual_labels, predictions):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(actual_labels, predictions):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"True Negatives (Correctly identified benign traffic): {cm[0,0]}\")\n",
    "print(f\"False Positives (Benign traffic misclassified as malicious): {cm[0,1]}\")\n",
    "print(f\"False Negatives (Malicious traffic misclassified as benign): {cm[1,0]}\")\n",
    "print(f\"True Positives (Correctly identified malicious traffic): {cm[1,1]}\")\n",
    "\n",
    "# Protocol-wise Performance\n",
    "print(\"\\nProtocol-wise Performance:\")\n",
    "print(\"-\" * 50)\n",
    "for protocol, stats in protocol_stats.iterrows():\n",
    "    print(f\"{protocol:10} - Actual: {stats['Actual_Rate']:.1f}% | \"\n",
    "          f\"Predicted: {stats['Predicted_Rate']:.1f}% | \"\n",
    "          f\"Count: {int(stats['Count'])}\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df['actual_label'] = actual_labels\n",
    "df['predicted_label'] = predictions\n",
    "df['prediction_probability'] = predictions_prob\n",
    "df.to_csv(\"data/comparison_results.csv\", index=False)\n",
    "\n",
    "# Save threshold analysis\n",
    "metrics_df.to_csv(\"data/threshold_analysis.csv\", index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"chosen_threshold\": float(chosen_threshold),\n",
    "    \"optimal_thresholds\": {\n",
    "        \"f1_score\": float(optimal_f1_threshold),\n",
    "        \"accuracy\": float(optimal_accuracy_threshold),\n",
    "        \"precision\": float(optimal_precision_threshold),\n",
    "        \"balanced\": float(balanced_threshold)\n",
    "    },\n",
    "    \"total_packets\": len(df),\n",
    "    \"actual_malicious\": int(actual_labels.sum()),\n",
    "    \"predicted_malicious\": int(predictions.sum()),\n",
    "    \"confusion_matrix\": {\n",
    "        \"true_negatives\": int(cm[0,0]),\n",
    "        \"false_positives\": int(cm[0,1]),\n",
    "        \"false_negatives\": int(cm[1,0]),\n",
    "        \"true_positives\": int(cm[1,1])\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy_score(actual_labels, predictions)),\n",
    "        \"precision\": float(precision_score(actual_labels, predictions)),\n",
    "        \"recall\": float(recall_score(actual_labels, predictions)),\n",
    "        \"f1_score\": float(f1_score(actual_labels, predictions)),\n",
    "        \"roc_auc\": float(roc_auc)\n",
    "    },\n",
    "    \"protocol_wise_stats\": protocol_stats.to_dict(),\n",
    "    \"analysis_timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open(\"data/comparison_metrics.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to 'data' directory\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39bcf2a5-abcc-49b8-a35f-d9e282aaaef8",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(actual_labels, predictions_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot main ROC curve\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "# Add diagonal line (random classifier)\n",
    "plt.plot(\n",
    "    [0, 1], [0, 1], color=\"navy\", linestyle=\"--\", label=\"Random Classifier (AUC = 0.5)\"\n",
    ")\n",
    "\n",
    "# Add key threshold points\n",
    "key_thresholds = [0.9, 0.7, 0.5, 0.3]\n",
    "colors = [\"red\", \"green\", \"blue\", \"purple\"]\n",
    "\n",
    "for threshold, color in zip(key_thresholds, colors):\n",
    "    # Find closest threshold value\n",
    "    idx = np.argmin(np.abs(thresholds - threshold))\n",
    "    plt.plot(\n",
    "        fpr[idx],\n",
    "        tpr[idx],\n",
    "        \"o\",\n",
    "        color=color,\n",
    "        markersize=10,\n",
    "        label=f\"Threshold = {threshold:.1f}\",\n",
    "    )\n",
    "\n",
    "    # Add annotation with actual values\n",
    "    plt.annotate(\n",
    "        f\"TPR: {tpr[idx]:.3f}\\nFPR: {fpr[idx]:.3f}\",\n",
    "        xy=(fpr[idx], tpr[idx]),\n",
    "        xytext=(10, -10),\n",
    "        textcoords=\"offset points\",\n",
    "        ha=\"left\",\n",
    "        va=\"top\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.7),\n",
    "        arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0\"),\n",
    "    )\n",
    "\n",
    "# Customize plot\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Receiver Operating Characteristic (ROC) Curve Analysis\\n\"\n",
    "    \"with Different Confidence Thresholds\",\n",
    "    pad=20,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "# Add explanatory text\n",
    "plt.text(\n",
    "    0.6,\n",
    "    0.2,\n",
    "    \"ROC Curve Interpretation:\\n\\n\"\n",
    "    \"- Closer to top-left corner = better performance\\n\"\n",
    "    \"- AUC = Area Under Curve (1.0 = perfect, 0.5 = random)\\n\"\n",
    "    \"- Higher threshold = fewer false positives\\n\"\n",
    "    \"- Lower threshold = fewer false negatives\",\n",
    "    bbox=dict(facecolor=\"white\", alpha=0.7),\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"\\nROC Curve Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"AUC Score: {roc_auc:.3f}\")\n",
    "print(\"\\nPerformance at key thresholds:\")\n",
    "for threshold in key_thresholds:\n",
    "    idx = np.argmin(np.abs(thresholds - threshold))\n",
    "    print(f\"\\nThreshold: {threshold:.1f}\")\n",
    "    print(f\"True Positive Rate: {tpr[idx]:.3f}\")\n",
    "    print(f\"False Positive Rate: {fpr[idx]:.3f}\")\n",
    "    print(f\"Specificity (True Negative Rate): {1-fpr[idx]:.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49428fe0-6605-4251-b385-bd4289042677",
   "metadata": {},
   "source": [
    "# After calculating metrics for each threshold, but before the confusion matrix:\n",
    "\n",
    "# Create performance comparison bar graph\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Data for the graph\n",
    "thresholds = [0.10, 0.30, 0.50, 0.70, 0.85, 0.90, 0.93, 0.95, 0.99]\n",
    "accuracies = [0.963, 0.957, 0.955, 0.958, 0.957, 0.958, 0.026, 0.038, 0.031]\n",
    "precisions = [0.974, 0.974, 0.974, 0.978, 0.978, 0.979, 0.458, 0.692, 0.562]\n",
    "recalls = [0.989, 0.982, 0.980, 0.980, 0.978, 0.977, 0.017, 0.016, 0.008]\n",
    "f1_scores = [0.981, 0.978, 0.977, 0.979, 0.978, 0.978, 0.033, 0.032, 0.015]\n",
    "\n",
    "x = np.arange(len(thresholds))\n",
    "width = 0.2  # Width of bars\n",
    "\n",
    "# Creating bars\n",
    "plt.bar(x - 1.5 * width, accuracies, width, label=\"Accuracy\", color=\"skyblue\")\n",
    "plt.bar(x - 0.5 * width, precisions, width, label=\"Precision\", color=\"lightgreen\")\n",
    "plt.bar(x + 0.5 * width, recalls, width, label=\"Recall\", color=\"salmon\")\n",
    "plt.bar(x + 1.5 * width, f1_scores, width, label=\"F1-Score\", color=\"mediumpurple\")\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlabel(\"Confidence Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\n",
    "    \"Performance Metrics Across Different Confidence Thresholds\",\n",
    "    pad=20,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.xticks(x, [f\"{t:.2f}\" for t in thresholds], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Ensure layout is tight and nothing is cut off\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Continue with your existing confusion matrix code..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4254bf1-76bb-4406-8466-dbe58133041c",
   "metadata": {},
   "source": [
    "# Create comparison bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "comparison_data = pd.DataFrame(\n",
    "    {\"Actual\": actual_labels, \"Predicted\": predictions.flatten()}\n",
    ")\n",
    "\n",
    "# Calculate percentages\n",
    "actual_pct = (\n",
    "    comparison_data[\"Actual\"].value_counts() / len(actual_labels) * 100\n",
    ").round(2)\n",
    "predicted_pct = (\n",
    "    comparison_data[\"Predicted\"].value_counts() / len(predictions) * 100\n",
    ").round(2)\n",
    "\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(\n",
    "    x - width / 2,\n",
    "    [actual_pct.get(0, 0), actual_pct.get(1, 0)],\n",
    "    width,\n",
    "    label=\"Actual\",\n",
    "    color=\"lightblue\",\n",
    ")\n",
    "plt.bar(\n",
    "    x + width / 2,\n",
    "    [predicted_pct.get(0, 0), predicted_pct.get(1, 0)],\n",
    "    width,\n",
    "    label=\"Predicted\",\n",
    "    color=\"lightcoral\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Traffic Type\")\n",
    "plt.ylabel(\"Percentage of Total Traffic\")\n",
    "plt.title(\n",
    "    f\"Actual vs Predicted Traffic Distribution\\nConfidence Threshold: {chosen_threshold:.2f}\",\n",
    "    pad=20,\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.xticks(x, [\"Benign Traffic\", \"Malicious Traffic\"])\n",
    "plt.legend()\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, v in enumerate([actual_pct.get(0, 0), actual_pct.get(1, 0)]):\n",
    "    plt.text(i - width / 2, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "for i, v in enumerate([predicted_pct.get(0, 0), predicted_pct.get(1, 0)]):\n",
    "    plt.text(i + width / 2, v, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fae8621-59ee-4c43-a0af-813aa2cf86d2",
   "metadata": {},
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "\n",
    "def find_latest_log_file(log_dir):\n",
    "    \"\"\"Find the latest event file in the log directory\"\"\"\n",
    "    files = glob.glob(\n",
    "        os.path.join(log_dir, \"**\", \"events.out.tfevents.*\"), recursive=True\n",
    "    )\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No event files found in {log_dir}\")\n",
    "    return max(files, key=os.path.getmtime)\n",
    "\n",
    "\n",
    "def extract_metrics_from_logs(event_file):\n",
    "    \"\"\"Extract metrics from TensorBoard event file\"\"\"\n",
    "    metrics = {\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_accuracy\": [],\n",
    "        \"steps\": [],\n",
    "    }\n",
    "    try:\n",
    "        for event in summary_iterator(event_file):\n",
    "            if event.HasField(\"summary\"):\n",
    "                for value in event.summary.value:\n",
    "                    if hasattr(value, \"simple_value\"):\n",
    "                        tag = value.tag\n",
    "                        if tag in [\"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"]:\n",
    "                            metrics[tag].append(value.simple_value)\n",
    "                            if tag == \"loss\":  # Only add step once per iteration\n",
    "                                metrics[\"steps\"].append(event.step)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading event file: {e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "try:\n",
    "    # Find and load the latest log file\n",
    "    log_dir = \"logs/fit\"\n",
    "    latest_event_file = find_latest_log_file(log_dir)\n",
    "    print(f\"Reading logs from: {latest_event_file}\")\n",
    "\n",
    "    # Extract metrics\n",
    "    metrics = extract_metrics_from_logs(latest_event_file)\n",
    "\n",
    "    if any(len(v) > 0 for v in metrics.values()):\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "\n",
    "        plt.style.use(\"default\")\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Plot Loss\n",
    "        if metrics[\"loss\"] and metrics[\"val_loss\"]:\n",
    "            ax1.plot(\n",
    "                metrics[\"steps\"], metrics[\"loss\"], label=\"Training Loss\", color=\"blue\"\n",
    "            )\n",
    "            ax1.plot(\n",
    "                metrics[\"steps\"],\n",
    "                metrics[\"val_loss\"],\n",
    "                label=\"Validation Loss\",\n",
    "                color=\"orange\",\n",
    "            )\n",
    "\n",
    "            # Find early stopping point\n",
    "            val_loss = np.array(metrics[\"val_loss\"])\n",
    "            min_loss_idx = np.argmin(val_loss)\n",
    "            ax1.axvline(\n",
    "                x=metrics[\"steps\"][min_loss_idx],\n",
    "                color=\"r\",\n",
    "                linestyle=\"--\",\n",
    "                label=f\"Early Stopping\\nEpoch {min_loss_idx}\",\n",
    "            )\n",
    "\n",
    "        ax1.set_title(\"Model Loss Over Time\", pad=20, fontsize=14, fontweight=\"bold\")\n",
    "        ax1.set_xlabel(\"Steps\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot Accuracy\n",
    "        if metrics[\"accuracy\"] and metrics[\"val_accuracy\"]:\n",
    "            ax2.plot(\n",
    "                metrics[\"steps\"],\n",
    "                metrics[\"accuracy\"],\n",
    "                label=\"Training Accuracy\",\n",
    "                color=\"blue\",\n",
    "            )\n",
    "            ax2.plot(\n",
    "                metrics[\"steps\"],\n",
    "                metrics[\"val_accuracy\"],\n",
    "                label=\"Validation Accuracy\",\n",
    "                color=\"orange\",\n",
    "            )\n",
    "            ax2.axvline(\n",
    "                x=metrics[\"steps\"][min_loss_idx],\n",
    "                color=\"r\",\n",
    "                linestyle=\"--\",\n",
    "                label=f\"Early Stopping\\nEpoch {min_loss_idx}\",\n",
    "            )\n",
    "\n",
    "        ax2.set_title(\n",
    "            \"Model Accuracy Over Time\", pad=20, fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        ax2.set_xlabel(\"Steps\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(\"\\nTraining Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        if metrics[\"loss\"]:\n",
    "            print(f\"Total steps: {len(metrics['steps'])}\")\n",
    "            print(\n",
    "                f\"Best validation loss: {min(metrics['val_loss']):.4f} at step {metrics['steps'][min_loss_idx]}\"\n",
    "            )\n",
    "            if metrics[\"accuracy\"]:\n",
    "                max_acc = max(metrics[\"accuracy\"])\n",
    "                max_acc_idx = np.argmax(metrics[\"accuracy\"])\n",
    "                print(\n",
    "                    f\"Best training accuracy: {max_acc:.4f} at step {metrics['steps'][max_acc_idx]}\"\n",
    "                )\n",
    "            if metrics[\"val_accuracy\"]:\n",
    "                max_val_acc = max(metrics[\"val_accuracy\"])\n",
    "                max_val_acc_idx = np.argmax(metrics[\"val_accuracy\"])\n",
    "                print(\n",
    "                    f\"Best validation accuracy: {max_val_acc:.4f} at step {metrics['steps'][max_val_acc_idx]}\"\n",
    "                )\n",
    "    else:\n",
    "        print(\"No metrics found in the log file.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\n",
    "        \"Please check if the log directory exists and contains TensorBoard event files.\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
