{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "import xgboost as xgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pyshark\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load training data\n",
    "data_train = pd.read_csv('data/KDDTrain+.txt')\n",
    "\n",
    "columns = (['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent',\n",
    "            'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root',\n",
    "            'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "            'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',\n",
    "            'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "            'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'outcome', 'level'])\n",
    "\n",
    "data_train.columns = columns\n",
    "data_train.loc[data_train['outcome'] == \"normal\", \"outcome\"] = 'normal'\n",
    "data_train.loc[data_train['outcome'] != 'normal', \"outcome\"] = 'attack'\n",
    "\n",
    "cat_cols = ['is_host_login', 'protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_guest_login', 'level', 'outcome']\n",
    "\n",
    "def Scaling(df_num, cols):\n",
    "    std_scaler = RobustScaler()\n",
    "    std_scaler_temp = std_scaler.fit_transform(df_num)\n",
    "    std_df = pd.DataFrame(std_scaler_temp, columns=cols)\n",
    "    return std_df\n",
    "\n",
    "def preprocess_with_padding(dataframe, original_columns):\n",
    "    # Preprocess the data as usual\n",
    "    dataframe['protocol_type'] = dataframe['protocol_type'].astype('category').cat.codes\n",
    "    df_num = dataframe.drop(cat_cols, axis=1, errors='ignore')\n",
    "    num_cols = df_num.columns\n",
    "    scaled_df = Scaling(df_num, num_cols)\n",
    "    dataframe.drop(labels=num_cols, axis=\"columns\", inplace=True)\n",
    "    dataframe[num_cols] = scaled_df[num_cols]\n",
    "\n",
    "    # Convert 'outcome' to binary labels\n",
    "    dataframe.loc[dataframe['outcome'] == \"normal\", \"outcome\"] = 0\n",
    "    dataframe.loc[dataframe['outcome'] != 0, \"outcome\"] = 1\n",
    "\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['protocol_type', 'service', 'flag'], drop_first=True)\n",
    "\n",
    "    # Add missing columns from the original training data\n",
    "    for col in original_columns:\n",
    "        if col not in dataframe.columns:\n",
    "            dataframe[col] = 0\n",
    "\n",
    "    # Ensure the columns are in the same order\n",
    "    dataframe = dataframe[original_columns]\n",
    "    return dataframe\n",
    "\n",
    "# Preprocess training data\n",
    "def preprocess(dataframe):\n",
    "    df_num = dataframe.drop(cat_cols, axis=1)\n",
    "    num_cols = df_num.columns\n",
    "    scaled_df = Scaling(df_num, num_cols)\n",
    "    dataframe.drop(labels=num_cols, axis=\"columns\", inplace=True)\n",
    "    dataframe[num_cols] = scaled_df[num_cols]\n",
    "\n",
    "    dataframe.loc[dataframe['outcome'] == \"normal\", \"outcome\"] = 0\n",
    "    dataframe.loc[dataframe['outcome'] != 0, \"outcome\"] = 1\n",
    "\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['protocol_type', 'service', 'flag'])\n",
    "    return dataframe\n",
    "\n",
    "scaled_train = preprocess(data_train)\n",
    "x = scaled_train.drop(['outcome', 'level'], axis=1).values.astype('float32')\n",
    "y = scaled_train['outcome'].values.astype('int')\n",
    "y_reg = scaled_train['level'].values\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca = pca.fit(x)\n",
    "x_reduced = pca.transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(x_train.shape[1:]),\n",
    "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                          bias_regularizer=regularizers.L2(1e-4),\n",
    "                          activity_regularizer=regularizers.L2(1e-5)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu',\n",
    "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                          bias_regularizer=regularizers.L2(1e-4),\n",
    "                          activity_regularizer=regularizers.L2(1e-5)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=512, activation='relu',\n",
    "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                          bias_regularizer=regularizers.L2(1e-4),\n",
    "                          activity_regularizer=regularizers.L2(1e-5)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu',\n",
    "                          kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                          bias_regularizer=regularizers.L2(1e-4),\n",
    "                          activity_regularizer=regularizers.L2(1e-5)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=1, verbose=1)\n",
    "\n",
    "# Load the pcap file\n",
    "capture = pyshark.FileCapture('/mnt/data/2018-10-25-14-06-32-192.168.1.132.pcap')\n",
    "\n",
    "# Process packets\n",
    "packet_data = []\n",
    "for packet in capture:\n",
    "    try:\n",
    "        packet_info = {\n",
    "            'duration': float(packet.sniff_time.timestamp()),\n",
    "            'protocol_type': packet.highest_layer,\n",
    "            'service': packet.transport_layer if hasattr(packet, 'transport_layer') else 'unknown',\n",
    "            'flag': packet.tcp.flags if hasattr(packet, 'tcp') else 0,\n",
    "            'src_bytes': int(packet.length),\n",
    "            'dst_bytes': 0,\n",
    "            'land': 1 if packet.ip.src == packet.ip.dst else 0,\n",
    "            # Add other fields as needed\n",
    "            'outcome': 0,\n",
    "            'level': 0\n",
    "        }\n",
    "        packet_data.append(packet_info)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame and preprocess\n",
    "df = pd.DataFrame(packet_data)\n",
    "original_feature_columns = list(scaled_train.drop(['outcome', 'level'], axis=1).columns)\n",
    "preprocessed_new_data = preprocess_with_padding(df, original_feature_columns)\n",
    "x_new = preprocessed_new_data.drop(['outcome', 'level'], axis=1, errors='ignore').values.astype('float32')\n",
    "\n",
    "# Transform with PCA and make predictions\n",
    "x_new_reduced = pca.transform(x_new)\n",
    "predictions = model.predict(x_new_reduced)\n",
    "predicted_outcomes = (predictions > 0.5).astype('int')\n",
    "\n",
    "# Save predictions\n",
    "df['predicted_outcome'] = predicted_outcomes\n",
    "df.to_csv('data/predicted_new_data.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'data/predicted_new_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
