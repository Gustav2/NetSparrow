{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:17:12.136772Z",
     "start_time": "2024-11-05T10:17:12.124644Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "3f0383a3334ccf8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:17:16.892417Z",
     "start_time": "2024-11-05T10:17:14.550572Z"
    }
   },
   "source": [
    "# Training files \n",
    "TRAIN_FILES = ['conn.log.labeled', \"conn2.log.labeled\"]\n",
    "\n",
    "for file in TRAIN_FILES:\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        with open(f'{file}.csv', 'w') as ff:\n",
    "            for line in lines[8:-1]:\n",
    "                line = line.replace('\\t', ',').replace('   ', ',')\n",
    "                ff.write(line)    "
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "855f96dc3571032a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:17:22.155139Z",
     "start_time": "2024-11-05T10:17:17.778316Z"
    }
   },
   "source": "data_train = pd.read_csv('conn.log.labeled.csv')",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   1525879831.015811  CUmrqr4svHuSXJy5z7  192.168.100.103  51524  \\\n",
       "0       1.525880e+09  CH98aB3s1kJeq6SFOc  192.168.100.103  56305   \n",
       "1       1.525880e+09   C3GBTkINvXNjVGtN5  192.168.100.103  41101   \n",
       "2       1.525880e+09   CDe43c1PtgynajGI6  192.168.100.103  60905   \n",
       "3       1.525880e+09  CJaDcG3MZzvf1YVYI4  192.168.100.103  44301   \n",
       "4       1.525880e+09  CMBrup3BLXivSp4Avc  192.168.100.103  50244   \n",
       "\n",
       "    65.127.233.163  23  tcp  -  2.999051  0 0.1  S0 -.1 -.2  0.2  S  3  180  \\\n",
       "0    63.150.16.171  23  tcp  -         -  -   -  S0   -   -    0  S  1   60   \n",
       "1     111.40.23.49  23  tcp  -         -  -   -  S0   -   -    0  S  1   60   \n",
       "2  131.174.215.147  23  tcp  -  2.998796  0   0  S0   -   -    0  S  3  180   \n",
       "3      91.42.47.63  23  tcp  -         -  -   -  S0   -   -    0  S  1   60   \n",
       "4  120.210.108.200  23  tcp  -         -  -   -  S0   -   -    0  S  1   60   \n",
       "\n",
       "   0.3  0.4  (empty)  Malicious  PartOfAHorizontalPortScan  \n",
       "0    0    0  (empty)  Malicious  PartOfAHorizontalPortScan  \n",
       "1    0    0  (empty)  Malicious  PartOfAHorizontalPortScan  \n",
       "2    0    0  (empty)  Malicious  PartOfAHorizontalPortScan  \n",
       "3    0    0  (empty)  Malicious  PartOfAHorizontalPortScan  \n",
       "4    0    0  (empty)  Malicious  PartOfAHorizontalPortScan  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1525879831.015811</th>\n",
       "      <th>CUmrqr4svHuSXJy5z7</th>\n",
       "      <th>192.168.100.103</th>\n",
       "      <th>51524</th>\n",
       "      <th>65.127.233.163</th>\n",
       "      <th>23</th>\n",
       "      <th>tcp</th>\n",
       "      <th>-</th>\n",
       "      <th>2.999051</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>S0</th>\n",
       "      <th>-.1</th>\n",
       "      <th>-.2</th>\n",
       "      <th>0.2</th>\n",
       "      <th>S</th>\n",
       "      <th>3</th>\n",
       "      <th>180</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>(empty)</th>\n",
       "      <th>Malicious</th>\n",
       "      <th>PartOfAHorizontalPortScan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CH98aB3s1kJeq6SFOc</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>56305</td>\n",
       "      <td>63.150.16.171</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>C3GBTkINvXNjVGtN5</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>41101</td>\n",
       "      <td>111.40.23.49</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CDe43c1PtgynajGI6</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>60905</td>\n",
       "      <td>131.174.215.147</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>2.998796</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CJaDcG3MZzvf1YVYI4</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>44301</td>\n",
       "      <td>91.42.47.63</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.525880e+09</td>\n",
       "      <td>CMBrup3BLXivSp4Avc</td>\n",
       "      <td>192.168.100.103</td>\n",
       "      <td>50244</td>\n",
       "      <td>120.210.108.200</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "2d3ddae86f202248",
   "metadata": {},
   "source": [
    "columns = ([\"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\", \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"tunnel_parents\", \"label\", \"detailed-label\"])\n",
    "\n",
    "data_train.columns = columns\n",
    "\n",
    "data_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ff2e8eee6b9fe7",
   "metadata": {},
   "source": [
    "data_train.describe().style.background_gradient(cmap='Blues').set_properties(**{'font-family':'Segoe UI'})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8597324da2b1307",
   "metadata": {},
   "source": [
    "def pie_plot(df, cols_list, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    for ax, col in zip(axes.ravel(), cols_list):\n",
    "        df[col].value_counts().plot(ax=ax, kind='pie', figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n",
    "        ax.set_title(str(col), fontsize = 12)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "pie_plot(data_train, [\"label\", \"proto\"], 1, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5c42be675045045",
   "metadata": {},
   "source": [
    "def Scaling(df_num, cols):\n",
    "    std_scaler = RobustScaler()\n",
    "    std_scaler_temp = std_scaler.fit_transform(df_num)\n",
    "    std_df = pd.DataFrame(std_scaler_temp, columns=cols)\n",
    "    return std_df\n",
    "\n",
    "\n",
    "def preprocess(dataframe):\n",
    "    cat_cols = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "    \n",
    "    dataframe = dataframe.drop(columns=[\"ts\", \"uid\", \"id.orig_h\", \"id.resp_h\", \"id.orig_p\", \"id.resp_p\", \"tunnel_parents\", \"detailed-label\"], errors='ignore')\n",
    "    \n",
    "    dataframe.replace('-', np.nan, inplace=True)\n",
    "    \n",
    "    numeric_cols = dataframe.columns.difference(cat_cols + [\"label\"])\n",
    "    for col in numeric_cols:\n",
    "        dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "    \n",
    "    df_num = dataframe[numeric_cols]\n",
    "\n",
    "    all_nan_cols = df_num.columns[df_num.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "        print(f\"Columns with all NaN values: {all_nan_cols}\")\n",
    "        df_num[all_nan_cols] = 0  \n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    imputed_values = imputer.fit_transform(df_num)\n",
    "\n",
    "    df_num = pd.DataFrame(imputed_values, columns=numeric_cols, index=dataframe.index)\n",
    "\n",
    "    scaled_df = Scaling(df_num, df_num.columns)\n",
    "\n",
    "    dataframe[df_num.columns] = scaled_df.values\n",
    "    \n",
    "    dataframe[\"label\"] = dataframe[\"label\"].apply(lambda x: 0 if x == \"Benign\" else 1)\n",
    "    \n",
    "    dataframe = pd.get_dummies(dataframe, columns=cat_cols, drop_first=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4a10981b08795be",
   "metadata": {},
   "source": [
    "scaled_train = preprocess(data_train)\n",
    "\n",
    "malicious_sample = scaled_train[scaled_train['label'] == 1].sample(5, random_state=42)\n",
    "normal_sample = scaled_train[scaled_train['label'] == 0].sample(5, random_state=42)\n",
    "\n",
    "x = scaled_train.drop(['label'], axis=1, errors='ignore').values.astype('float32')\n",
    "y = scaled_train['label'].values.astype('int')\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "x_reduced = pca.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_reduced, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_packet_classifier(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Creates an optimized deep learning model for network packet classification\n",
    "    with improved architecture and regularization.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input features\n",
    "        num_classes: Number of output classes (default 1 for binary classification)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize regularization parameters\n",
    "    reg_config = {\n",
    "        'kernel': regularizers.L1L2(l1=1e-6, l2=1e-5),\n",
    "        'bias': regularizers.L2(1e-5),\n",
    "        'activity': regularizers.L2(1e-6)\n",
    "    }\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # Input layer with batch normalization\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # First block - smaller layers for feature extraction\n",
    "        layers.Dense(32, activation='relu',\n",
    "                    kernel_regularizer=reg_config['kernel'],\n",
    "                    bias_regularizer=reg_config['bias'],\n",
    "                    activity_regularizer=reg_config['activity']),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Second block - moderate size for pattern recognition\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_regularizer=reg_config['kernel'],\n",
    "                    bias_regularizer=reg_config['bias'],\n",
    "                    activity_regularizer=reg_config['activity']),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Third block - larger size for complex pattern learning\n",
    "        layers.Dense(256, activation='relu',\n",
    "                    kernel_regularizer=reg_config['kernel'],\n",
    "                    bias_regularizer=reg_config['bias'],\n",
    "                    activity_regularizer=reg_config['activity']),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, batch_size=32, max_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains the model with optimized parameters and callbacks.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping to prevent overfitting\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Reduce learning rate when training plateaus\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Save best model with .keras extension\n",
    "        ModelCheckpoint(\n",
    "            'best_packet_classifier.keras',  # Changed from .h5 to .keras\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Compile model with optimized parameters\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3,\n",
    "        clipnorm=1.0  # Gradient clipping to prevent exploding gradients\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Example usage:\n",
    "def preprocess_data(columns, data):\n",
    "    \"\"\"\n",
    "    Preprocesses the network packet data.\n",
    "    \"\"\"\n",
    "    # Convert categorical columns to numerical\n",
    "    categorical_columns = ['proto', 'service', 'conn_state', 'history', 'tunnel_parents']\n",
    "    numerical_columns = [col for col in columns if col not in categorical_columns + ['label', 'detailed-label']]\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        data = pd.get_dummies(data, columns=[col], prefix=col)\n",
    "    \n",
    "    return data\n"
   ],
   "id": "68a875bdb63b7c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "model = create_packet_classifier(input_shape)\n",
    "history = train_model(model, x_train, y_train, x_test, y_test)"
   ],
   "id": "4c484f89da5be307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('test3.log.labeled', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    with open('test3.log.labeled.csv', 'w') as ff:\n",
    "        for line in lines[8:-1]:\n",
    "            line = line.replace('\\t', ',').replace('   ', ',')\n",
    "            ff.write(line)        "
   ],
   "id": "ce5c4b0a8d5ec071"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('test3.log.labeled.csv')\n",
    "\n",
    "columns = [\"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\", \n",
    "          \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\", \n",
    "          \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\", \n",
    "          \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \n",
    "          \"resp_ip_bytes\", \"tunnel_parents\", \"label\", \"detailed-label\"]\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "required_columns = [\"proto\", \"service\", \"conn_state\", \"history\", \"local_orig\", \n",
    "                   \"local_resp\", \"orig_bytes\", \"resp_bytes\", \"orig_ip_bytes\", \n",
    "                   \"resp_ip_bytes\", \"resp_pkts\"]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "df_new = df.copy()\n",
    "\n",
    "# Add history column with default value if missing\n",
    "if 'history' not in df.columns:\n",
    "    df['history'] = 'S'\n",
    "    \n",
    "\n",
    "\n",
    "# Add label column (will be dropped during preprocessing)\n",
    "df['label'] = 'unknown'\n",
    "\n",
    "# Preprocess the data using your existing preprocess function\n",
    "preprocessed_data = preprocess(df)\n",
    "\n",
    "# Ensure columns match training data\n",
    "train_columns = scaled_train.drop('label', axis=1).columns\n",
    "missing_cols = set(train_columns) - set(preprocessed_data.columns)\n",
    "for col in missing_cols:\n",
    "    preprocessed_data[col] = 0\n",
    "\n",
    "# Reorder columns to match training data\n",
    "preprocessed_data = preprocessed_data[train_columns]\n",
    "\n",
    "# Convert to numpy array\n",
    "x_new = preprocessed_data.values.astype('float32')\n",
    "\n",
    "# Transform with PCA\n",
    "x_new_reduced = pca.transform(x_new)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_new_reduced)\n",
    "predicted_outcomes = (predictions > 0.5).astype('int')\n",
    "\n",
    "# Add predictions to original DataFrame\n",
    "df['predicted_outcome'] = predicted_outcomes\n",
    "\n",
    "# Calculate basic statistics\n",
    "total_packets = len(predicted_outcomes)\n",
    "malicious_packets = np.sum(predicted_outcomes)\n",
    "normal_packets = total_packets - malicious_packets\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\"-\"*55)\n",
    "\n",
    "print(\"Prediction Summary:\")\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\"*55)\n",
    "\n",
    "# Actual summary\n",
    "print(\"Summary of actual data\")\n",
    "print(\"Total packets: \", len(df_new))\n",
    "print(\"Unique protocols: \", df_new['proto'].nunique())\n",
    "\n",
    "malicious_packets = df_new['label'].value_counts()[0]\n",
    "normal_packets = df_new['label'].value_counts()[1]\n",
    "total_packets = len(df_new)\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\" \")\n",
    "# Print the statistics\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\"*55)\n",
    "\n",
    "pie_plot(df_new, [\"label\", \"proto\"], 1, 2)\n",
    "\n",
    "\n",
    "# Create time-based analysis\n",
    "df['timestamp'] = pd.to_datetime(df['ts'], unit='s')\n",
    "df['minute'] = df['timestamp'].dt.floor('T')\n",
    "\n",
    "# Analyze predictions over time\n",
    "temporal_analysis = df.groupby('minute').agg({\n",
    "    'predicted_outcome': ['count', 'sum']\n",
    "}).reset_index()\n",
    "temporal_analysis.columns = ['minute', 'total_packets', 'malicious_packets']\n",
    "temporal_analysis['normal_packets'] = temporal_analysis['total_packets'] - temporal_analysis['malicious_packets']\n",
    "temporal_analysis['malicious_ratio'] = temporal_analysis['malicious_packets'] / temporal_analysis['total_packets']\n",
    "\n",
    "# Visualize results\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Timeline of predictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(temporal_analysis['minute'], temporal_analysis['normal_packets'],\n",
    "        label='Normal', color='#2ecc71', alpha=0.7, linewidth=2)\n",
    "plt.plot(temporal_analysis['minute'], temporal_analysis['malicious_packets'],\n",
    "        label='Malicious', color='#e74c3c', alpha=0.7, linewidth=2)\n",
    "plt.title('Packet Classification Over Time', pad=20, fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Number of Packets', fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Plot 2: Malicious ratio over time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(temporal_analysis['minute'], temporal_analysis['malicious_ratio'],\n",
    "        color='#9b59b6', alpha=0.7, linewidth=2)\n",
    "plt.title('Ratio of Malicious Packets Over Time', pad=20, fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Time', fontsize=10)\n",
    "plt.ylabel('Ratio of Malicious Packets', fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "os.makedirs('data', exist_ok=True)\n",
    "df.to_csv('data/predicted_new_data.csv', index=False)\n",
    "temporal_analysis.to_csv('data/temporal_analysis.csv', index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_packets': total_packets,\n",
    "    'malicious_packets': int(malicious_packets),\n",
    "    'normal_packets': int(normal_packets),\n",
    "    'malicious_percentage': float(malicious_percentage),\n",
    "    'analysis_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'available_features': df.columns.tolist()\n",
    "}\n",
    "\n",
    "with open('data/verification_results.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to 'data' directory\")"
   ],
   "id": "43320917c0491b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def clean_duration(x):\n",
    "    \"\"\"Clean duration values by handling various formats and invalid values\"\"\"\n",
    "    if pd.isna(x) or x == '-' or str(x).strip() == '':\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return float(x)\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NaT\n",
    "\n",
    "def create_timeline_plots(temporal_analysis):\n",
    "    \"\"\"\n",
    "    Create timeline visualizations of the network traffic with improved error handling\n",
    "    and date formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert minute column to datetime if it's not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(temporal_analysis['minute']):\n",
    "            temporal_analysis['minute'] = pd.to_datetime(temporal_analysis['minute'])\n",
    "\n",
    "        # Sort by time to ensure proper plotting\n",
    "        temporal_analysis = temporal_analysis.sort_values('minute')\n",
    "\n",
    "        # Create figure and axes objects explicitly\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "        # Plot 1: Timeline of predictions\n",
    "        ax1.plot(temporal_analysis['minute'], temporal_analysis['normal_packets'],\n",
    "                label='Normal', color='#2ecc71', alpha=0.7, linewidth=2)\n",
    "        ax1.plot(temporal_analysis['minute'], temporal_analysis['malicious_packets'],\n",
    "                label='Malicious', color='#e74c3c', alpha=0.7, linewidth=2)\n",
    "        ax1.set_title('Packet Classification Over Time', pad=20, fontsize=12, fontweight='bold')\n",
    "        ax1.set_xlabel('Time', fontsize=10)\n",
    "        ax1.set_ylabel('Number of Packets', fontsize=10)\n",
    "        ax1.legend(frameon=True, fancybox=True, shadow=True)\n",
    "        ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Plot 2: Malicious ratio over time\n",
    "        ax2.plot(temporal_analysis['minute'], temporal_analysis['malicious_ratio'],\n",
    "                color='#9b59b6', alpha=0.7, linewidth=2)\n",
    "        ax2.set_title('Ratio of Malicious Packets Over Time', pad=20, fontsize=12, fontweight='bold')\n",
    "        ax2.set_xlabel('Time', fontsize=10)\n",
    "        ax2.set_ylabel('Ratio of Malicious Packets', fontsize=10)\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "         \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "        # Close the figure to free up memory\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating timeline plots: {str(e)}\")\n",
    "        print(\"Debug information:\")\n",
    "        print(f\"Temporal analysis columns: {temporal_analysis.columns.tolist()}\")\n",
    "        print(f\"Minute column dtype: {temporal_analysis['minute'].dtype}\")\n",
    "        print(\"\\nFirst few rows of temporal analysis data:\")\n",
    "        print(temporal_analysis.head())\n",
    "\n",
    "def create_protocol_plot(protocol_analysis):\n",
    "    \"\"\"Create protocol distribution visualization with improved error handling\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Create bar plot\n",
    "        ax = protocol_analysis[['normal_packets', 'malicious_packets']].plot(\n",
    "            kind='bar',\n",
    "            stacked=True,\n",
    "            color=['#2ecc71', '#e74c3c']\n",
    "        )\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title('Packet Distribution by Protocol', fontsize=12, fontweight='bold')\n",
    "        plt.xlabel('Protocol', fontsize=10)\n",
    "        plt.ylabel('Number of Packets', fontsize=10)\n",
    "        plt.legend(['Normal', 'Malicious'], frameon=True, fancybox=True, shadow=True)\n",
    "        \n",
    "        # Rotate x-axis labels and adjust their position\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for c in ax.containers:\n",
    "            ax.bar_label(c, fmt='%.0f', label_type='center')\n",
    "\n",
    "        # Add grid to y-axis\n",
    "        plt.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "        \n",
    "        # Adjust layout to prevent label cutoff\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "        \n",
    "        # Close the figure to free up memory\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating protocol plot: {str(e)}\")\n",
    "        print(\"Debug information:\")\n",
    "        print(f\"Protocol analysis columns: {protocol_analysis.columns.tolist()}\")\n",
    "        print(\"\\nFirst few rows of protocol analysis data:\")\n",
    "        print(protocol_analysis.head())\n",
    "\n",
    "def analyze_network_traffic(df):\n",
    "    \"\"\"Analyze network traffic data and generate visualizations and statistics\"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate basic statistics about predictions\n",
    "    total_packets = len(df)\n",
    "    predicted_outcomes = df['predicted_outcome'].values\n",
    "    malicious_packets = np.sum(predicted_outcomes)\n",
    "    normal_packets = total_packets - malicious_packets\n",
    "    malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "    print(\"Prediction Summary:\")\n",
    "    print(f\"Total packets analyzed: {total_packets}\")\n",
    "    print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "    print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Create time-based analysis with error handling\n",
    "    temporal_analysis = None\n",
    "    try:\n",
    "        if 'duration' in df.columns:\n",
    "            # Clean duration values\n",
    "            df['clean_duration'] = df['duration'].apply(clean_duration)\n",
    "            \n",
    "            # Count and report invalid duration values\n",
    "            invalid_count = df['clean_duration'].isna().sum()\n",
    "            if invalid_count > 0:\n",
    "                print(f\"Warning: Found {invalid_count} invalid duration values\")\n",
    "            \n",
    "            # Create timestamp for valid duration values\n",
    "            df['timestamp'] = pd.to_datetime(\n",
    "                df['clean_duration'].apply(\n",
    "                    lambda x: datetime.fromtimestamp(x) if pd.notnull(x) else pd.NaT\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Only proceed with temporal analysis if we have valid timestamps\n",
    "            valid_timestamps = df['timestamp'].notna()\n",
    "            if valid_timestamps.any():\n",
    "                df['minute'] = df['timestamp'].dt.floor('T')\n",
    "                \n",
    "                # Analyze predictions over time (using only valid timestamps)\n",
    "                temporal_analysis = df[valid_timestamps].groupby('minute').agg({\n",
    "                    'predicted_outcome': ['count', 'sum']\n",
    "                }).reset_index()\n",
    "                temporal_analysis.columns = ['minute', 'total_packets', 'malicious_packets']\n",
    "                temporal_analysis['normal_packets'] = temporal_analysis['total_packets'] - temporal_analysis['malicious_packets']\n",
    "                temporal_analysis['malicious_ratio'] = temporal_analysis['malicious_packets'] / temporal_analysis['total_packets']\n",
    "\n",
    "                print(f\"\\nTemporal analysis completed using {valid_timestamps.sum()} valid timestamps\")\n",
    "                \n",
    "                # Create visualizations only if we have enough data points\n",
    "                if len(temporal_analysis) > 1:\n",
    "                    create_timeline_plots(temporal_analysis)\n",
    "                else:\n",
    "                    print(\"Warning: Not enough valid timestamps for temporal visualization\")\n",
    "            else:\n",
    "                print(\"Warning: No valid timestamps found for temporal analysis\")\n",
    "        else:\n",
    "            print(\"Warning: No 'duration' column found in the dataset\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not perform temporal analysis: {str(e)}\")\n",
    "\n",
    "    # Protocol analysis\n",
    "    if 'protocol_type' in df.columns:\n",
    "        try:\n",
    "            protocol_analysis = df.groupby('protocol_type').agg({\n",
    "                'predicted_outcome': ['count', 'sum', 'mean']\n",
    "            })\n",
    "            protocol_analysis.columns = ['total_packets', 'malicious_packets', 'malicious_ratio']\n",
    "            protocol_analysis['normal_packets'] = protocol_analysis['total_packets'] - protocol_analysis['malicious_packets']\n",
    "            \n",
    "            print(\"\\nProtocol-based Analysis:\")\n",
    "            print(protocol_analysis.round(4))\n",
    "            \n",
    "            create_protocol_plot(protocol_analysis)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not complete protocol analysis: {str(e)}\")\n",
    "\n",
    "    # Packet size analysis with improved error handling\n",
    "    analyze_packet_sizes(df)\n",
    "\n",
    "    # Anomaly detection\n",
    "    if temporal_analysis is not None:\n",
    "        detect_anomalies(temporal_analysis)\n",
    "\n",
    "    # Generate and save summary\n",
    "    save_results(df, total_packets, malicious_packets, normal_packets, \n",
    "                malicious_percentage, temporal_analysis)\n",
    "\n",
    "def analyze_packet_sizes(df):\n",
    "    \"\"\"Analyze packet sizes for normal and malicious traffic\"\"\"\n",
    "    for size_col in ['src_bytes', 'dst_bytes']:\n",
    "        if size_col in df.columns:\n",
    "            try:\n",
    "                # Clean the byte values first\n",
    "                df[f'clean_{size_col}'] = pd.to_numeric(df[size_col], errors='coerce')\n",
    "                normal_mean = df[df['predicted_outcome'] == 0][f'clean_{size_col}'].mean()\n",
    "                malicious_mean = df[df['predicted_outcome'] == 1][f'clean_{size_col}'].mean()\n",
    "                \n",
    "                print(f\"\\n{size_col.replace('_', ' ').title()} Analysis:\")\n",
    "                print(f\"Average size of normal packets: {normal_mean:.2f}\")\n",
    "                print(f\"Average size of malicious packets: {malicious_mean:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not analyze {size_col}: {str(e)}\")\n",
    "\n",
    "def detect_anomalies(temporal_analysis):\n",
    "    \"\"\"Detect potential anomalies in the traffic patterns\"\"\"\n",
    "    print(\"\\nPotential Anomalies:\")\n",
    "    try:\n",
    "        spike_threshold = temporal_analysis['malicious_ratio'].mean() + 2 * temporal_analysis['malicious_ratio'].std()\n",
    "        spikes = temporal_analysis[temporal_analysis['malicious_ratio'] > spike_threshold]\n",
    "        if not spikes.empty:\n",
    "            print(f\"Found {len(spikes)} time periods with unusual spikes in malicious traffic:\")\n",
    "            for _, spike in spikes.iterrows():\n",
    "                print(f\"Time: {spike['minute']}, Malicious ratio: {spike['malicious_ratio']:.2%}\")\n",
    "        else:\n",
    "            print(\"No significant spikes in malicious traffic detected.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not complete anomaly detection: {str(e)}\")\n",
    "\n",
    "def save_results(df, total_packets, malicious_packets, normal_packets, \n",
    "                malicious_percentage, temporal_analysis):\n",
    "    \"\"\"Save analysis results to files\"\"\"\n",
    "    summary_stats = {\n",
    "        'total_packets': total_packets,\n",
    "        'malicious_packets': int(malicious_packets),\n",
    "        'normal_packets': int(normal_packets),\n",
    "        'malicious_percentage': float(malicious_percentage),\n",
    "        'analysis_timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'available_features': df.columns.tolist()\n",
    "    }\n",
    "\n",
    "    if 'clean_duration' in df.columns:\n",
    "        valid_durations = df['clean_duration'].dropna()\n",
    "        if not valid_durations.empty:\n",
    "            summary_stats['total_duration_seconds'] = float(valid_durations.max() - valid_durations.min())\n",
    "            summary_stats['valid_duration_count'] = len(valid_durations)\n",
    "            summary_stats['invalid_duration_count'] = len(df) - len(valid_durations)\n",
    "\n",
    "    try:\n",
    "        os.makedirs('data', exist_ok=True)\n",
    "        \n",
    "        with open('data/verification_results.json', 'w') as f:\n",
    "            json.dump(summary_stats, f, indent=4)\n",
    "        print(\"\\nVerification results saved to 'data/verification_results.json'\")\n",
    "        \n",
    "        if temporal_analysis is not None:\n",
    "            temporal_analysis.to_csv('data/temporal_analysis.csv', index=False)\n",
    "            print(\"Temporal analysis saved to 'data/temporal_analysis.csv'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error saving results: {str(e)}\")"
   ],
   "id": "43d40f4e592030ff",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
