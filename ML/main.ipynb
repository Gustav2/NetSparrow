{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b17099-c43b-482e-9c85-7d3300f5928d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:19.259119Z",
     "start_time": "2024-11-05T10:30:13.007577Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipympl in /opt/conda/lib/python3.12/site-packages (0.9.4)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.12/site-packages (from ipympl) (0.2.0)\n",
      "Requirement already satisfied: ipython<9 in /opt/conda/lib/python3.12/site-packages (from ipympl) (8.29.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /opt/conda/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
      "Requirement already satisfied: matplotlib<4,>=3.4.0 in /opt/conda/lib/python3.12/site-packages (from ipympl) (3.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from ipympl) (2.0.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from ipympl) (11.0.0)\n",
      "Requirement already satisfied: traitlets<6 in /opt/conda/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython<9->ipympl) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.12/site-packages (8.29.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.12/site-packages (from ipython) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.12/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.12/site-packages (from stack-data->ipython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.12/site-packages (18.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting dask\n",
      "  Downloading dask-2024.10.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: click>=8.1 in /opt/conda/lib/python3.12/site-packages (from dask) (8.1.7)\n",
      "Collecting cloudpickle>=3.0.0 (from dask)\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from dask) (24.1)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.12/site-packages (from dask) (6.0.2)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting locket (from partd>=1.4.0->dask)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Downloading dask-2024.10.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: toolz, locket, fsspec, cloudpickle, partd, dask\n",
      "Successfully installed cloudpickle-3.1.0 dask-2024.10.0 fsspec-2024.10.0 locket-1.0.0 partd-1.4.2 toolz-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install tensorflow\n",
    "%pip install numpy\n",
    "%pip install ipympl\n",
    "%pip install ipython\n",
    "%pip install pyarrow\n",
    "%pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:19.259119Z",
     "start_time": "2024-11-05T10:30:13.007577Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4f6eff9-eb5f-4dee-bd4d-1f1201d8b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "nblog = open(\"nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cb98d3648c0add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:19.278325Z",
     "start_time": "2024-11-05T10:30:19.275130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training files\n",
    "TRAIN_FILES = [\n",
    "    \"conn.log.labeled\",\n",
    "    \"conn2.log.labeled\",\n",
    "    \"conn3.log.labeled\",\n",
    "    \"conn4.log.labeled\",\n",
    "    \"conn5.log.labeled\",\n",
    "]\n",
    "\n",
    "columns = [\n",
    "    \"ts\",\n",
    "    \"uid\",\n",
    "    \"id.orig_h\",\n",
    "    \"id.orig_p\",\n",
    "    \"id.resp_h\",\n",
    "    \"id.resp_p\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0383a3334ccf8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:21:17.659963Z",
     "start_time": "2024-11-05T10:21:11.208572Z"
    }
   },
   "outputs": [],
   "source": [
    "for file in TRAIN_FILES:\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        with open(f\"{file}.csv\", \"w\") as ff:\n",
    "            for line in lines[8:-1]:\n",
    "                line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "                ff.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562663b-694e-4651-87e3-d617b6d9d163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:32.086018Z",
     "start_time": "2024-11-05T10:30:19.994741Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_train = pd.concat([pd.read_csv(f'{file}.csv', names=columns) for file in TRAIN_FILES], ignore_index=True)\n",
    "\n",
    "\n",
    "def load_csv_parallel(file, columns):\n",
    "    \"\"\"Optimized CSV loading function for large files\"\"\"\n",
    "    chunks = pd.read_csv(\n",
    "        f\"{file}.csv\",\n",
    "        names=columns,\n",
    "        engine=\"c\",\n",
    "        low_memory=False,\n",
    "        memory_map=True,\n",
    "        cache_dates=True,\n",
    "        chunksize=1_000_000,  # Process 1M rows at a time\n",
    "    )\n",
    "    return pd.concat(chunks, ignore_index=True, copy=False)\n",
    "\n",
    "\n",
    "# Optimize for 64 cores, leaving 2 cores free for system processes\n",
    "num_cores = 62\n",
    "\n",
    "# Create process pool\n",
    "with mp.Pool(num_cores) as pool:\n",
    "    # Map the loading function across files\n",
    "    dataframes = pool.starmap(\n",
    "        load_csv_parallel, [(file, columns) for file in TRAIN_FILES]\n",
    "    )\n",
    "\n",
    "# Combine all files' dataframes\n",
    "data_train = pd.concat(dataframes, ignore_index=True, copy=False)\n",
    "\n",
    "# Clear unneeded variables to free memory\n",
    "del dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d3ddae86f202248",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:32.118630Z",
     "start_time": "2024-11-05T10:30:32.102327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>uid</th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_h</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>resp_bytes</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>local_orig</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>history</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>tunnel_parents</th>\n",
       "      <th>label</th>\n",
       "      <th>detailed-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1545336612.05354</td>\n",
       "      <td>CddD2d1qU4UCgtFGV1</td>\n",
       "      <td>192.168.1.197</td>\n",
       "      <td>123</td>\n",
       "      <td>217.30.75.147</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>Benign</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1545336612.053552</td>\n",
       "      <td>CWsvajFlOE7fg2py4</td>\n",
       "      <td>192.168.1.197</td>\n",
       "      <td>123</td>\n",
       "      <td>94.124.107.190</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.006726</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>Benign</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1545336620.053729</td>\n",
       "      <td>Cthriq2tzQhGjBK304</td>\n",
       "      <td>192.168.1.197</td>\n",
       "      <td>123</td>\n",
       "      <td>89.221.214.130</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>Benign</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1545336629.053585</td>\n",
       "      <td>CC2GtA46zw7L9veEI4</td>\n",
       "      <td>192.168.1.197</td>\n",
       "      <td>123</td>\n",
       "      <td>81.2.254.224</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>Benign</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1545336642.053662</td>\n",
       "      <td>CozNZa1gT9JUxCsqz2</td>\n",
       "      <td>192.168.1.197</td>\n",
       "      <td>123</td>\n",
       "      <td>81.2.248.189</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>-</td>\n",
       "      <td>Benign</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ts                 uid      id.orig_h  id.orig_p  \\\n",
       "0   1545336612.05354  CddD2d1qU4UCgtFGV1  192.168.1.197        123   \n",
       "1  1545336612.053552   CWsvajFlOE7fg2py4  192.168.1.197        123   \n",
       "2  1545336620.053729  Cthriq2tzQhGjBK304  192.168.1.197        123   \n",
       "3  1545336629.053585  CC2GtA46zw7L9veEI4  192.168.1.197        123   \n",
       "4  1545336642.053662  CozNZa1gT9JUxCsqz2  192.168.1.197        123   \n",
       "\n",
       "        id.resp_h id.resp_p proto service  duration orig_bytes resp_bytes  \\\n",
       "0   217.30.75.147       123   udp       -  0.002241         48         48   \n",
       "1  94.124.107.190       123   udp       -  0.006726         48         48   \n",
       "2  89.221.214.130       123   udp       -  0.003734         48         48   \n",
       "3    81.2.254.224       123   udp       -  0.004739         48         48   \n",
       "4    81.2.248.189       123   udp       -  0.004739         48         48   \n",
       "\n",
       "  conn_state local_orig local_resp  missed_bytes history  orig_pkts  \\\n",
       "0         SF          -          -             0      Dd          1   \n",
       "1         SF          -          -             0      Dd          1   \n",
       "2         SF          -          -             0      Dd          1   \n",
       "3         SF          -          -             0      Dd          1   \n",
       "4         SF          -          -             0      Dd          1   \n",
       "\n",
       "   orig_ip_bytes resp_pkts resp_ip_bytes tunnel_parents   label detailed-label  \n",
       "0             76         1            76              -  Benign              -  \n",
       "1             76         1            76              -  Benign              -  \n",
       "2             76         1            76              -  Benign              -  \n",
       "3             76         1            76              -  Benign              -  \n",
       "4             76         1            76              -  Benign              -  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff2e8eee6b9fe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:28:51.761921Z",
     "start_time": "2024-11-05T10:28:49.853009Z"
    }
   },
   "outputs": [],
   "source": [
    "data_train.describe().style.background_gradient(cmap=\"Blues\").set_properties(\n",
    "    **{\"font-family\": \"Segoe UI\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597324da2b1307",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T10:30:33.106975Z",
     "start_time": "2024-11-05T10:30:32.212649Z"
    }
   },
   "outputs": [],
   "source": [
    "def pie_plot(df, cols_list, rows, cols):\n",
    "    fig, axes = plt.subplots(rows, cols)\n",
    "    for ax, col in zip(axes.ravel(), cols_list):\n",
    "        df[col].value_counts().plot(\n",
    "            ax=ax, kind=\"pie\", figsize=(15, 15), fontsize=10, autopct=\"%1.0f%%\"\n",
    "        )\n",
    "        ax.set_title(str(col), fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "pie_plot(data_train, [\"label\", \"proto\"], 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d014c100-0863-4fce-ae61-fdd974833451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaling(df_num, cols):\n",
    "   \"\"\"Optimized scaling function with progress tracking\"\"\"\n",
    "   print(f\"Starting RobustScaler on {len(cols)} columns...\")\n",
    "   t0 = time.time()\n",
    "   \n",
    "   # Initialize and fit scaler\n",
    "   scaler = RobustScaler(copy=True)\n",
    "   scaled_values = scaler.fit_transform(df_num)\n",
    "   \n",
    "   # Convert to DataFrame\n",
    "   scaled_df = pd.DataFrame(scaled_values, columns=cols, index=df_num.index)\n",
    "   \n",
    "   print(f\"Scaling completed in {time.time() - t0:.2f}s\")\n",
    "   return scaled_df\n",
    "\n",
    "def preprocess(dataframe):\n",
    "   \"\"\"Optimized preprocessing pipeline with detailed progress tracking\"\"\"\n",
    "   print(\"\\n    Starting preprocessing pipeline...\")\n",
    "   print(f\"    Initial dataframe shape: {dataframe.shape}\")\n",
    "   t_start = time.time()\n",
    "   \n",
    "   # Define columns to process\n",
    "   cat_cols = [\"proto\", \"service\", \"conn_state\", \"history\"]\n",
    "   drop_cols = [\n",
    "       \"ts\", \"uid\", \"id.orig_h\", \"id.resp_h\", \"id.orig_p\", \n",
    "       \"id.resp_p\", \"tunnel_parents\", \"detailed-label\"\n",
    "   ]\n",
    "   \n",
    "   # 1. Drop unnecessary columns\n",
    "   print(\"\\n    [1/7] Dropping unnecessary columns...\")\n",
    "   t0 = time.time()\n",
    "   dataframe = dataframe.drop(columns=drop_cols, errors=\"ignore\")\n",
    "   print(f\"    Columns dropped in {time.time() - t0:.2f}s\")\n",
    "   print(f\"    Shape after dropping: {dataframe.shape}\")\n",
    "   \n",
    "   # 2. Replace dashes with NaN\n",
    "   print(\"\\n    [2/7] Replacing dashes with NaN...\")\n",
    "   t0 = time.time()\n",
    "   dataframe.replace(\"-\", np.nan, inplace=True)\n",
    "   print(f\"    Replacement completed in {time.time() - t0:.2f}s\")\n",
    "   \n",
    "   # 3. Process numeric columns\n",
    "   print(\"\\n    [3/7] Processing numeric columns...\")\n",
    "   t0 = time.time()\n",
    "   numeric_cols = dataframe.columns.difference(cat_cols + [\"label\"])\n",
    "   print(f\"    Found {len(numeric_cols)} numeric columns\")\n",
    "   \n",
    "   # Convert to numeric in chunks for better memory usage\n",
    "   chunk_size = 5\n",
    "   for i in range(0, len(numeric_cols), chunk_size):\n",
    "       chunk_cols = numeric_cols[i:i+chunk_size]\n",
    "       for col in chunk_cols:\n",
    "           dataframe[col] = pd.to_numeric(dataframe[col], errors=\"coerce\")\n",
    "           \n",
    "   print(f\"    Numeric conversion completed in {time.time() - t0:.2f}s\")\n",
    "   \n",
    "   # 4. Handle numeric data\n",
    "   print(\"\\n    [4/7] Processing numeric data and handling NaN values...\")\n",
    "   t0 = time.time()\n",
    "   df_num = dataframe[numeric_cols]\n",
    "   \n",
    "   # Check for all-NaN columns\n",
    "   all_nan_cols = df_num.columns[df_num.isna().all()]\n",
    "   if len(all_nan_cols) > 0:\n",
    "       print(f\"    Found {len(all_nan_cols)} columns with all NaN values\")\n",
    "       df_num[all_nan_cols] = 0\n",
    "   \n",
    "   # Impute missing values\n",
    "   print(\"Imputing missing values...\")\n",
    "   imputer = SimpleImputer(strategy=\"mean\", copy=False)\n",
    "   imputed_values = imputer.fit_transform(df_num)\n",
    "   df_num = pd.DataFrame(imputed_values, columns=numeric_cols, index=dataframe.index)\n",
    "   print(f\"    Numeric processing completed in {time.time() - t0:.2f}s\")\n",
    "   \n",
    "   # 5. Scale numeric data\n",
    "   print(\"\\n    [5/7] Scaling numeric data...\")\n",
    "   scaled_df = Scaling(df_num, df_num.columns)\n",
    "   dataframe[df_num.columns] = scaled_df.values\n",
    "   del scaled_df  # Free memory\n",
    "   \n",
    "   # 6. Convert labels\n",
    "   print(\"\\n    [6/7] Converting labels...\")\n",
    "   t0 = time.time()\n",
    "   dataframe[\"label\"] = (dataframe[\"label\"] != \"Benign\").astype(np.int8)  # More efficient than lambda\n",
    "   print(f\"    Label conversion completed in {time.time() - t0:.2f}s\")\n",
    "   \n",
    "   # 7. One-hot encode categorical columns\n",
    "   print(\"\\n    [7/7] One-hot encoding categorical columns...\")\n",
    "   t0 = time.time()\n",
    "   dataframe = pd.get_dummies(dataframe, columns=cat_cols, drop_first=True, sparse=False)\n",
    "   print(f\"    One-hot encoding completed in {time.time() - t0:.2f}s\")\n",
    "   \n",
    "   # Final statistics\n",
    "   print(f\"\\n    Final dataframe shape: {dataframe.shape}\")\n",
    "   print(f\"    Memory usage: {dataframe.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "   print(f\"    Total preprocessing time: {time.time() - t_start:.2f}s\")\n",
    "   \n",
    "   return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9e92dc-f200-4a77-ac63-bf61363e555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7] Starting preprocessing pipeline...\n",
      "Input shape: (125600735, 23)\n",
      "[2/7] Preprocessing data...\n",
      "\n",
      "    Starting preprocessing pipeline...\n",
      "    Initial dataframe shape: (125600735, 23)\n",
      "\n",
      "    [1/7] Dropping unnecessary columns...\n",
      "    Columns dropped in 13.09s\n",
      "    Shape after dropping: (125600735, 15)\n",
      "\n",
      "    [2/7] Replacing dashes with NaN...\n",
      "    Replacement completed in 147.37s\n",
      "\n",
      "    [3/7] Processing numeric columns...\n",
      "    Found 10 numeric columns\n",
      "    Numeric conversion completed in 122.08s\n",
      "\n",
      "    [4/7] Processing numeric data and handling NaN values...\n",
      "    Found 1 columns with all NaN values\n",
      "Imputing missing values...\n",
      "    Numeric processing completed in 91.04s\n",
      "\n",
      "    [5/7] Scaling numeric data...\n",
      "Starting RobustScaler on 10 columns...\n",
      "Scaling completed in 39.56s\n",
      "\n",
      "    [6/7] Converting labels...\n",
      "    Label conversion completed in 7.05s\n",
      "\n",
      "    [7/7] One-hot encoding categorical columns...\n",
      "    One-hot encoding completed in 141.45s\n",
      "\n",
      "    Final dataframe shape: (125600735, 254)\n",
      "    Memory usage: 38809.43 MB\n",
      "    Total preprocessing time: 566.55s\n",
      "Preprocessing completed in 566.58s\n",
      "Preprocessed shape: (125600735, 254)\n",
      "[3/7] Converting features to float32...\n"
     ]
    }
   ],
   "source": [
    "print(f\"[1/7] Starting preprocessing pipeline...\")\n",
    "print(f\"Input shape: {data_train.shape}\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Preprocess data with progress tracking\n",
    "print(\"[2/7] Preprocessing data...\")\n",
    "scaled_train = preprocess(data_train)\n",
    "print(f\"Preprocessing completed in {time.time() - t0:.2f}s\")\n",
    "print(f\"Preprocessed shape: {scaled_train.shape}\")\n",
    "\n",
    "# Convert to float32 and split features/target\n",
    "print(\"[3/7] Converting features to float32...\")\n",
    "t1 = time.time()\n",
    "x = scaled_train.drop([\"label\"], axis=1, errors=\"ignore\").values\n",
    "x = np.asarray(x, dtype=np.float32)  # More efficient than .astype()\n",
    "print(f\"Features conversion completed in {time.time() - t1:.2f}s\")\n",
    "print(f\"Features shape: {x.shape}\")\n",
    "\n",
    "print(\"[4/7] Converting labels to int32...\")\n",
    "t2 = time.time()\n",
    "y = np.asarray(scaled_train[\"label\"].values, dtype=np.int32)  # int32 is usually sufficient\n",
    "print(f\"Labels conversion completed in {time.time() - t2:.2f}s\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Free memory\n",
    "print(\"[5/7] Clearing unused data to free memory...\")\n",
    "del scaled_train\n",
    "del data_train\n",
    "\n",
    "# PCA with progress info\n",
    "print(\"[6/7] Performing PCA reduction to 20 components...\")\n",
    "t3 = time.time()\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "x_reduced = pca.fit_transform(x)\n",
    "print(f\"PCA completed in {time.time() - t3:.2f}s\")\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"Reduced features shape: {x_reduced.shape}\")\n",
    "\n",
    "# Free more memory\n",
    "del x\n",
    "\n",
    "# Train test split\n",
    "print(\"[7/7] Performing train-test split...\")\n",
    "t4 = time.time()\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "   x_reduced, \n",
    "   y, \n",
    "   test_size=0.2, \n",
    "   random_state=42,\n",
    "   shuffle=True,  # Explicit shuffle\n",
    "   stratify=y     # Maintain label distribution\n",
    ")\n",
    "print(f\"Split completed in {time.time() - t4:.2f}s\")\n",
    "\n",
    "# Print final shapes\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTotal pipeline time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "# Memory cleanup\n",
    "del x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a875bdb63b7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "\n",
    "def create_packet_classifier(input_shape, num_classes=1):\n",
    "    \"\"\"\n",
    "    Creates an optimized deep learning model for network packet classification\n",
    "    with improved architecture and regularization.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Shape of input features\n",
    "        num_classes: Number of output classes (default 1 for binary classification)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize regularization parameters\n",
    "    reg_config = {\n",
    "        \"kernel\": regularizers.L1L2(l1=1e-6, l2=1e-5),\n",
    "        \"bias\": regularizers.L2(1e-5),\n",
    "        \"activity\": regularizers.L2(1e-6),\n",
    "    }\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Input layer with batch normalization\n",
    "            layers.InputLayer(input_shape=input_shape),\n",
    "            layers.BatchNormalization(),\n",
    "            # First block - smaller layers for feature extraction\n",
    "            layers.Dense(\n",
    "                32,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            # Second block - moderate size for pattern recognition\n",
    "            layers.Dense(\n",
    "                64,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            # Third block - larger size for complex pattern learning\n",
    "            layers.Dense(\n",
    "                256,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=reg_config[\"kernel\"],\n",
    "                bias_regularizer=reg_config[\"bias\"],\n",
    "                activity_regularizer=reg_config[\"activity\"],\n",
    "            ),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            # Output layer\n",
    "            layers.Dense(num_classes, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, batch_size=32, max_epochs=50):\n",
    "    \"\"\"\n",
    "    Trains the model with optimized parameters and callbacks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        # Early stopping to prevent overfitting\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        # Reduce learning rate when training plateaus\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
    "        ),\n",
    "        # Save best model with .keras extension\n",
    "        ModelCheckpoint(\n",
    "            \"best_packet_classifier.keras\",  # Changed from .h5 to .keras\n",
    "            monitor=\"val_accuracy\",\n",
    "            save_best_only=True,\n",
    "            verbose=1,\n",
    "        ),\n",
    "        TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "    ]\n",
    "\n",
    "    # Compile model with optimized parameters\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=1e-3,\n",
    "        clipnorm=1.0,  # Gradient clipping to prevent exploding gradients\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            \"accuracy\",\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        batch_size=batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "def preprocess_data(columns, data):\n",
    "    \"\"\"\n",
    "    Preprocesses the network packet data.\n",
    "    \"\"\"\n",
    "    # Convert categorical columns to numerical\n",
    "    categorical_columns = [\n",
    "        \"proto\",\n",
    "        \"service\",\n",
    "        \"conn_state\",\n",
    "        \"history\",\n",
    "        \"tunnel_parents\",\n",
    "    ]\n",
    "    numerical_columns = [\n",
    "        col\n",
    "        for col in columns\n",
    "        if col not in categorical_columns + [\"label\", \"detailed-label\"]\n",
    "    ]\n",
    "\n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        data = pd.get_dummies(data, columns=[col], prefix=col)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c484f89da5be307",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "print(\"Shaped\")\n",
    "model = create_packet_classifier(input_shape)\n",
    "print(\"model\")\n",
    "history = train_model(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c4b0a8d5ec071",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test3.log.labeled\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    with open(\"test3.log.labeled.csv\", \"w\") as ff:\n",
    "        for line in lines[8:-1]:\n",
    "            line = line.replace(\"\\t\", \",\").replace(\"   \", \",\")\n",
    "            ff.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43320917c0491b7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test3.log.labeled.csv\")\n",
    "\n",
    "columns = [\n",
    "    \"ts\",\n",
    "    \"uid\",\n",
    "    \"id.orig_h\",\n",
    "    \"id.orig_p\",\n",
    "    \"id.resp_h\",\n",
    "    \"id.resp_p\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]\n",
    "\n",
    "df.columns = columns\n",
    "\n",
    "required_columns = [\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"conn_state\",\n",
    "    \"history\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "]\n",
    "\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = np.nan\n",
    "\n",
    "df_new = df.copy()\n",
    "\n",
    "# Add history column with default value if missing\n",
    "if \"history\" not in df.columns:\n",
    "    df[\"history\"] = \"S\"\n",
    "\n",
    "\n",
    "# Add label column (will be dropped during preprocessing)\n",
    "df[\"label\"] = \"unknown\"\n",
    "\n",
    "# Preprocess the data using your existing preprocess function\n",
    "preprocessed_data = preprocess(df)\n",
    "\n",
    "# Ensure columns match training data\n",
    "train_columns = scaled_train.drop(\"label\", axis=1).columns\n",
    "missing_cols = set(train_columns) - set(preprocessed_data.columns)\n",
    "for col in missing_cols:\n",
    "    preprocessed_data[col] = 0\n",
    "\n",
    "# Reorder columns to match training data\n",
    "preprocessed_data = preprocessed_data[train_columns]\n",
    "\n",
    "# Convert to numpy array\n",
    "x_new = preprocessed_data.values.astype(\"float32\")\n",
    "\n",
    "# Transform with PCA\n",
    "x_new_reduced = pca.transform(x_new)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_new_reduced)\n",
    "predicted_outcomes = (predictions > 0.5).astype(\"int\")\n",
    "\n",
    "# Add predictions to original DataFrame\n",
    "df[\"predicted_outcome\"] = predicted_outcomes\n",
    "\n",
    "# Calculate basic statistics\n",
    "total_packets = len(predicted_outcomes)\n",
    "malicious_packets = np.sum(predicted_outcomes)\n",
    "normal_packets = total_packets - malicious_packets\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "print(\"Prediction Summary:\")\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# Actual summary\n",
    "print(\"Summary of actual data\")\n",
    "print(\"Total packets: \", len(df_new))\n",
    "print(\"Unique protocols: \", df_new[\"proto\"].nunique())\n",
    "\n",
    "malicious_packets = df_new[\"label\"].value_counts()[0]\n",
    "normal_packets = df_new[\"label\"].value_counts()[1]\n",
    "total_packets = len(df_new)\n",
    "malicious_percentage = (malicious_packets / total_packets) * 100\n",
    "\n",
    "print(\" \")\n",
    "# Print the statistics\n",
    "print(f\"Total packets analyzed: {total_packets}\")\n",
    "print(f\"Malicious packets detected: {malicious_packets} ({malicious_percentage:.2f}%)\")\n",
    "print(f\"Normal packets detected: {normal_packets} ({100-malicious_percentage:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "\n",
    "pie_plot(df_new, [\"label\", \"proto\"], 1, 2)\n",
    "\n",
    "\n",
    "# Create time-based analysis\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"ts\"], unit=\"s\")\n",
    "df[\"minute\"] = df[\"timestamp\"].dt.floor(\"T\")\n",
    "\n",
    "# Analyze predictions over time\n",
    "temporal_analysis = (\n",
    "    df.groupby(\"minute\").agg({\"predicted_outcome\": [\"count\", \"sum\"]}).reset_index()\n",
    ")\n",
    "temporal_analysis.columns = [\"minute\", \"total_packets\", \"malicious_packets\"]\n",
    "temporal_analysis[\"normal_packets\"] = (\n",
    "    temporal_analysis[\"total_packets\"] - temporal_analysis[\"malicious_packets\"]\n",
    ")\n",
    "temporal_analysis[\"malicious_ratio\"] = (\n",
    "    temporal_analysis[\"malicious_packets\"] / temporal_analysis[\"total_packets\"]\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "plt.style.use(\"default\")\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Timeline of predictions\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"normal_packets\"],\n",
    "    label=\"Normal\",\n",
    "    color=\"#2ecc71\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"malicious_packets\"],\n",
    "    label=\"Malicious\",\n",
    "    color=\"#e74c3c\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.title(\"Packet Classification Over Time\", pad=20, fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Time\", fontsize=10)\n",
    "plt.ylabel(\"Number of Packets\", fontsize=10)\n",
    "plt.legend(frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Plot 2: Malicious ratio over time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(\n",
    "    temporal_analysis[\"minute\"],\n",
    "    temporal_analysis[\"malicious_ratio\"],\n",
    "    color=\"#9b59b6\",\n",
    "    alpha=0.7,\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.title(\n",
    "    \"Ratio of Malicious Packets Over Time\", pad=20, fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Time\", fontsize=10)\n",
    "plt.ylabel(\"Ratio of Malicious Packets\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(\"data/predicted_new_data.csv\", index=False)\n",
    "temporal_analysis.to_csv(\"data/temporal_analysis.csv\", index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    \"total_packets\": total_packets,\n",
    "    \"malicious_packets\": int(malicious_packets),\n",
    "    \"normal_packets\": int(normal_packets),\n",
    "    \"malicious_percentage\": float(malicious_percentage),\n",
    "    \"analysis_timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"available_features\": df.columns.tolist(),\n",
    "}\n",
    "\n",
    "with open(\"data/verification_results.json\", \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to 'data' directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
